"use strict";(self.webpackChunkpackit_dev=self.webpackChunkpackit_dev||[]).push([[2846],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>m});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=c(r),d=a,m=u["".concat(l,".").concat(d)]||u[d]||h[d]||o;return r?n.createElement(m,s(s({ref:t},p),{},{components:r})):n.createElement(m,s({ref:t},p))}));function m(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,s=new Array(o);s[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[u]="string"==typeof e?e:a,s[1]=i;for(var c=2;c<o;c++)s[c]=r[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,r)}d.displayName="MDXCreateElement"},74509:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var n=r(87462),a=(r(67294),r(3905));const o={title:"Workers scaling",authors:"jpopelka"},s=void 0,i={unversionedId:"deployment/workers-scaling",id:"deployment/workers-scaling",title:"Workers scaling",description:"What",source:"@site/research/deployment/workers-scaling.md",sourceDirName:"deployment",slug:"/deployment/workers-scaling",permalink:"/research/deployment/workers-scaling",draft:!1,editUrl:"https://github.com/packit/research/tree/main/research/deployment/workers-scaling.md",tags:[],version:"current",frontMatter:{title:"Workers scaling",authors:"jpopelka"},sidebar:"autogenerated",previous:{title:"Deployment and testing strategies",permalink:"/research/deployment/verification"},next:{title:"Deprecation policy",permalink:"/research/deprecation/"}},l={},c=[{value:"What",id:"what",level:2},{value:"History",id:"history",level:2},{value:"worker --concurrency",id:"worker---concurrency",level:2},{value:"Processes vs. threads aka. Celery execution pool",id:"processes-vs-threads-aka-celery-execution-pool",level:3},{value:"worker --autoscale",id:"worker---autoscale",level:3},{value:"Kubernetes/Openshift Horizontal Pod Autoscaling",id:"kubernetesopenshift-horizontal-pod-autoscaling",level:2},{value:"Summary/Suggestion",id:"summarysuggestion",level:2}],p={toc:c},u="wrapper";function h(e){let{components:t,...r}=e;return(0,a.kt)(u,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"what"},"What"),(0,a.kt)("p",null,"This research is about scaling worker(s) that do short-running tasks to ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/research/tree/main/improve-service-processing#problematic-parts-of-the-process"},"improve Packit Service event processing"),"."),(0,a.kt)("h2",{id:"history"},"History"),(0,a.kt)("p",null,"Once upon a time there were more (two?) workers who were not picky and took\nwhatever task they find in the (only) queue.\nTo not waste time they always assigned more (4) tasks at a time to themselves\nbefore starting to work on a task.\nIt ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/packit-service/issues/375"},"could then happen"),"\nthat one worker processed a ",(0,a.kt)("inlineCode",{parentName:"p"},"copr_build_finished")," event sooner than the other\nworker processed a corresponding ",(0,a.kt)("inlineCode",{parentName:"p"},"copr_build_started")," event,\nand then it looked like that the copr build never finished."),(0,a.kt)("p",null,"For example, if you have 2 workers and a task queue like"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"build1_start | build2_start | build3_start | build1_end | build2_end | build3_end\n")),(0,a.kt)("p",null,"and each worker takes 4 tasks, then the first worker takes"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"build1_start | build2_start | build3_start | build1_end\n")),(0,a.kt)("p",null,"and the second is left with"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"build2_end | build3_end\n")),(0,a.kt)("p",null,"and by the time the first worker starts processing ",(0,a.kt)("inlineCode",{parentName:"p"},"build2_start"),",\nthe second has already processed ",(0,a.kt)("inlineCode",{parentName:"p"},"build2_end")," so it looks like the build2 never ended."),(0,a.kt)("p",null,"This ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/packit-service/commit/5d199cfee54dafd9a5cd5dadc1086e15d78598e5"},"has been fixed"),"\nby making sure that the workers are not so greedy and reserve only one task ahead\n(",(0,a.kt)("inlineCode",{parentName:"p"},"--prefetch-multiplier=1"),", default is 4)."),(0,a.kt)("p",null,"Later, we also ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/packit-service/commit/617c76ef77ef37bc220b029bc49326d3841c54c0"},"set up 2 Celery queues, for long and short running tasks"),"\nbecause that's what ",(0,a.kt)("a",{parentName:"p",href:"https://docs.celeryq.dev/en/stable/userguide/optimizing.html#optimizing-prefetch-limit"},"Celery User Guide suggests"),"."),(0,a.kt)("h2",{id:"worker---concurrency"},"worker --concurrency"),(0,a.kt)("p",null,"The same ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/packit-service/commit/5d199cfee54dafd9a5cd5dadc1086e15d78598e5"},"commit"),"\nthat set ",(0,a.kt)("inlineCode",{parentName:"p"},"--prefetch-multiplier=1")," also set ",(0,a.kt)("inlineCode",{parentName:"p"},"--concurrency=1"),"\n(number of concurrent worker processes/threads executing tasks)\neven it's probably had no effect because ",(0,a.kt)("a",{parentName:"p",href:"https://docs.celeryq.dev/en/stable/userguide/configuration.html#std-setting-worker_concurrency"},'the default is "number of CPUs/cores on the host"'),"\nand the workers have no more than one core.\nWe definitely don't want to change the prefetch multiplier because that might\ncause ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/packit-service/issues/375"},"p-s#375"),"\nfor no significant benefit, but having more concurrent worker threads\nshould hopefully be safe."),(0,a.kt)("p",null,"Whether it's really safe depends on how Celery\nfetches messages for the worker's threads. If it waits for all threads\nto finish their tasks before fetching a new batch of messages (times prefetch-multiplier)\nfor all threads then we might still hit ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/packit-service/issues/375"},"p-s#375"),"\nif we have more workers serving the short-running-tasks queue.\nBut if Celery fetches a new message(s) for a thread once it finishes a task\nwithout waiting for other threads then we should be fine (as long as prefetch-multiplier=1).\nI checked the ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/celery/celery"},"Celery source code"),"\nbut couldn't find an answer for how/when it fetches messages for the threads\n(whether individually or for all at once). Someone's up for a challenge?;)\nBut if we had only one worker (with more threads) serving the\nshort-running-tasks queue then we should be fine anyway."),(0,a.kt)("h3",{id:"processes-vs-threads-aka-celery-execution-pool"},"Processes vs. threads aka. ",(0,a.kt)("a",{parentName:"h3",href:"https://www.distributedpython.com/2018/10/26/celery-execution-pool"},"Celery execution pool")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Processes")," (",(0,a.kt)("inlineCode",{parentName:"li"},"prefork")," pool) -\nare good when tasks are CPU bound. The number of processes should not exceed\nthe number of CPUs on host."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Threads")," (",(0,a.kt)("a",{parentName:"li",href:"https://docs.celeryq.dev/en/latest/userguide/concurrency/eventlet.html"},"eventlet"),"/",(0,a.kt)("a",{parentName:"li",href:"https://www.gevent.org"},"gevent")," pool) -\nwhen tasks are I/O or network bound. The number of greenlets (green threads) is\nunrelated to the number of CPUs on host and can go even to thousands.")),(0,a.kt)("h3",{id:"worker---autoscale"},"worker --autoscale"),(0,a.kt)("p",null,"Regarding the ",(0,a.kt)("inlineCode",{parentName:"p"},"--concurrency")," value, we even don't have to set a static number\nif we use ",(0,a.kt)("a",{parentName:"p",href:"https://docs.celeryq.dev/en/latest/userguide/workers.html#autoscaling"},"worker --autoscale"),"\nwhich dynamically resizes the pool based on load (",(0,a.kt)("a",{parentName:"p",href:"https://github.com/celery/celery/blob/aa9fd8a6c06e69c7eda2a59866c3d84622c85d20/celery/worker/autoscale.py#L85"},"number of requests"),"\nin the message queue, not a CPU load)."),(0,a.kt)("h2",{id:"kubernetesopenshift-horizontal-pod-autoscaling"},"Kubernetes/Openshift Horizontal Pod Autoscaling"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://docs.openshift.com/container-platform/4.10/nodes/pods/nodes-pods-autoscaling.html"},"Automatically scaling pods with the horizontal pod autoscaler ")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale"},"Horizontal Pod Autoscaling"))),(0,a.kt)("p",null,"The horizontal pod autoscaler (HPA) can be used to specify how should K8s/Openshift\nscale a StatefulSet based on metrics collected from the pods.\nThe supported metrics are CPU and/or memory utilization."),(0,a.kt)("p",null,"For example: You say that you want to have 1 to 5 workers with average CPU\nutilization around 50% (of their cpu resource request). The HPA controller\nthen periodically checks the cpu metrics and scales the pods up/down\nbased on the CPU consumption."),(0,a.kt)("p",null,"The memory utilization of short-running workers is constant so that's off the table.\nCPU utilization doesn't quite follow the load (of tasks in queue) because the tasks\nare mostly network bound. For example at times when there's not much to do a\nshort-running worker consumes about 2 millicores, while when there's a lots of tasks\nthe utilization jumps between 2 and 20 millicores, so it's hard to decide on a\ndesired average utilization."),(0,a.kt)("p",null,"The ideal metric for us would be a number of outstanding tasks in a queue,\nhowever ",(0,a.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects"},"external metrics"),"\nare not clearly described how to achieve."),(0,a.kt)("p",null,"We'd also need to return ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/packit/deployment/pull/142"},"readiness probes"),"\nto make the HPA work."),(0,a.kt)("p",null,"Compared to ",(0,a.kt)("inlineCode",{parentName:"p"},"worker --autoscale")," the HPA looks like:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"less reliable (if we used the CPU metrics)"),(0,a.kt)("li",{parentName:"ul"},"more work to set up (if we used custom/external metrics - queue size)"),(0,a.kt)("li",{parentName:"ul"},"slower - starting a pod is way slower than adding a thread"),(0,a.kt)("li",{parentName:"ul"},"more resource hungry - running 4 pods eats much more memory than running 1 pod with 4 threads")),(0,a.kt)("h2",{id:"summarysuggestion"},"Summary/Suggestion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Default for stg & dev can stay as it is, i.e. ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/packit/deployment/blob/cb47f9e6f806f9e340669031c7eb58df02b164f1/playbooks/deploy.yml#L40"},"one worker for all tasks"),"\nwith no concurrency/autoscaling/prefetching."),(0,a.kt)("li",{parentName:"ul"},"For prod, we should experiment with setting ",(0,a.kt)("inlineCode",{parentName:"li"},"--autoscale")," (or just ",(0,a.kt)("inlineCode",{parentName:"li"},"--concurrency"),")\ntogether with ",(0,a.kt)("inlineCode",{parentName:"li"},"--pool=eventlet/gevent"),". We should have only ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/packit/deployment/blob/cb47f9e6f806f9e340669031c7eb58df02b164f1/vars/packit/prod_template.yml#L77"},"one worker for serving\nshort-running tasks"),"\nwhich would autoscale based on load and ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/packit/deployment/blob/cb47f9e6f806f9e340669031c7eb58df02b164f1/vars/packit/prod_template.yml#L76"},"no worker for all tasks"),"\nto avoid ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/packit/packit-service/issues/375"},"p-s#375")," regression.")))}h.isMDXComponent=!0}}]);