"use strict";(self.webpackChunkpackit_dev=self.webpackChunkpackit_dev||[]).push([[11150],{64320:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/openscanhub-prototype","metadata":{"permalink":"/posts/openscanhub-prototype","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/openscanhub-prototype/index.md","source":"@site/posts/openscanhub-prototype/index.md","title":"SAST using OpenScanHub is here!","description":"We are excited to announce a new experimental feature in our service: the integration of Static","date":"2024-08-05T06:46:18.000Z","formattedDate":"August 5, 2024","tags":[{"label":"copr","permalink":"/posts/tags/copr"},{"label":"srpm","permalink":"/posts/tags/srpm"},{"label":"sast","permalink":"/posts/tags/sast"},{"label":"openscanhub","permalink":"/posts/tags/openscanhub"},{"label":"shift left","permalink":"/posts/tags/shift-left"}],"readingTime":2.265,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"SAST using OpenScanHub is here!","date":"2024-08-05T06:46:18.000Z","authors":"lbarczio","tags":["copr","srpm","sast","openscanhub","shift left"]},"nextItem":{"title":"Do you like your changelogs? What DevConf.CZ attendees think","permalink":"/posts/changelogs"}},"content":"We are excited to announce a new experimental feature in our service: the integration of Static\\nApplication Security Testing (SAST) using [OpenScanHub](https://openscanhub.fedoraproject.org/).\\n\\nOpenScanHub is a service that runs various static analyzers on RPM packages - by default `Cppcheck`,\\n`ShellCheck` and the static analyzers embedded in `GCC`.\\n\\nLet\'s have a look at the details of the prototype!\\n\\n\x3c!--truncate--\x3e\\n\\n## Why\\n\\nThis initiative contributes to the _shift left_ effort, aiming to detect and address security\\nvulnerabilities earlier in the development process, thus enhancing overall software quality\\nand security. By implementing this scanning functionality for `fedora-rawhide`, the current\\ndevelopment version of Fedora, we aim to catch security issues at the earliest possible stage.\\nThis is also important as Fedora serves as an upstream platform for downstream distributions\\nlike RHEL, ensuring that any vulnerabilities are addressed before they propagate to these other systems.\\n\\n## Functionality\\n\\nThe functionality is designed to automatically\\nscan for vulnerabilities and issues in code submitted through pull requests.\\nIt is controlled using the configuration option\\n[`osh_diff_scan_after_copr_build`](/docs/configuration#osh_diff_scan_after_copr_build),\\nwhich is currently set to `true` by default.\\nHowever, even with this option enabled, the differential scanning feature in OpenScanHub\\nwill only run for users who have also configured Copr builds with the `trigger: commit`\\nsetting and have matching target branches in their pull request and job configurations.\\nThis setup is crucial as it allows Packit to access the base build necessary for differential\\nscanning, leading to more precise detection of issues introduced by the pull request.\\n\\n## Setup\\n\\nTo utilise this feature, you must ensure that Copr builds for commits are configured,\\nalong with Copr builds for pull requests, both set to run for `fedora-rawhide-x86_64`.\\nIf you haven\'t enabled Copr builds for commits yet, you can add the following job\\nconfiguration to the `jobs` section in your Packit configuration:\\n\\n```yaml\\n- job: copr_build\\n  trigger: commit\\n  branch: main\\n  targets:\\n    - fedora-rawhide\\n  preserve_project: true\\n```\\n\\nThis configuration assumes that you merge your pull requests into the `main` branch\\nand ensures preserving the Copr project (as opposed to creating a temporary one,\\nsee more details [here](https://packit.dev/docs/configuration/upstream/copr_build#optional-parameters)).\\nYou can also build in your custom Copr project (more details\\n[here](https://packit.dev/docs/configuration/upstream/copr_build#using-a-custom-copr-project)).\\n\\nHere is an example of a scan showing some new findings:\\n\\n![Example findings](openscanhub-findings.png)\\n\\n:::tip\\n\\nBesides this newly introduced integration with Packit Service, you can also submit scans locally using Packit CLI,\\nsee more details in [docs](/docs/cli/scan-in-osh).\\n\\n:::\\n\\n## Conclusion\\n\\nAs this is an initial prototype, the configuration and functionality may evolve based on user feedback\\n(e.g. reporting the actual results of a scan).\\nWe invite you to try out this feature and [share your thoughts](https://github.com/packit/packit/discussions/2371) with us\\n(e.g. if you would benefit from other `osh-cli` options to be included).\\nFor more information on setting up this feature, see the [info above](#setup)."},{"id":"/changelogs","metadata":{"permalink":"/posts/changelogs","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/changelogs/index.md","source":"@site/posts/changelogs/index.md","title":"Do you like your changelogs? What DevConf.CZ attendees think","description":"Last month, we had the pleasure of engaging with a dynamic audience during our interactive talk on changelogs at the DevConf in Brno, Czech Republic. In case you missed it, you can watch the recording here. Throughout the session, we explored various aspects of changelog usage, including their content, format, and the potential for automation. By asking a series of questions to the attendees, we gathered insights and opinions that highlighted both common practices and divergent viewpoints within the community. In this follow-up article, we aim to summarise the key findings from our discussion, analyse the trends and preferences that emerged, and offer our reflections on the role of changelogs in software development.","date":"2024-07-01T00:00:00.000Z","formattedDate":"July 1, 2024","tags":[],"readingTime":8.45,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Lachman","email":"flachman@redhat.com","url":"https://github.com/lachmanfrantisek","imageURL":"https://github.com/lachmanfrantisek.png","key":"flachman"},{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"Do you like your changelogs? What DevConf.CZ attendees think","date":"2024-07-01T00:00:00.000Z","authors":["flachman","lbarczio"]},"prevItem":{"title":"SAST using OpenScanHub is here!","permalink":"/posts/openscanhub-prototype"},"nextItem":{"title":"DevConf.CZ 2024 and week around for Packit","permalink":"/posts/devconf-2024"}},"content":"Last month, we had the pleasure of engaging with a dynamic audience during our interactive talk on changelogs at the [DevConf](https://www.devconf.info/cz/) in Brno, Czech Republic. In case you missed it, you can watch the recording [here](https://youtu.be/TSifrKWNQT0?si=kRlu7PcQZwSwpy0R). Throughout the session, we explored various aspects of changelog usage, including their content, format, and the potential for automation. By asking a series of questions to the attendees, we gathered insights and opinions that highlighted both common practices and divergent viewpoints within the community. In this follow-up article, we aim to summarise the key findings from our discussion, analyse the trends and preferences that emerged, and offer our reflections on the role of changelogs in software development.\\n\\n![foto from the conference 1](./img/devconf_changelogs1.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n![foto from the conference 2](./img/devconf_changelogs2.jpg)\\n\\n## Content\\n\\nOne of the first questions we posed to our audience during the talk was, _\\"What do you, as a user, like to see in changelogs?\\"_ The most popular elements that the audience showed to like to see in changelogs were _breaking changes_ and _new features_. Interest in breaking changes indicates that users prioritise being informed about changes that might disrupt their current setup or workflow. As for the features, this shows a strong interest in understanding the latest enhancements and functionalities added to the software. Users appreciate knowing what new capabilities they can leverage. On a similar note, the information about deprecated functionality is also highly valuable for the audience. This can help users plan migrations and avoid using obsolete features. The audience also expressed a clear desire to understand the purpose and context of a new release. Responses such as _\\"why\\"_, _\\"purpose\\"_, _\\"reasons\\"_, _\\"relations\\"_, and _\\"changes motivation\\"_ highlight this need. Understanding the rationale behind changes helps users comprehend the development trajectory and decision-making process. Another interesting response was the desire to know _\\"am I affected\\"_, indicating that users primarily care about whether they need to take action or can safely ignore the update. There was also a response _\\"not a copy of commit msgs\\"_. But we will get into that later\u2026\\n\\n## Formats\\n\\nThe question that followed was, _\\"What format do you prefer for changelogs?\\"_ The most popular format by far was _Markdown_. Its simplicity, readability, and widespread use in the developer community make it highly appealing. Markdown\'s flexibility in formatting and ease of conversion to other formats also contribute to its popularity. This preference may also indicate its frequent use in blog posts or other articles, as Markdown can be easily rendered.\\n\\n_Plain text_ files were also a prominent choice. We assume this could be for their simplicity and universal compatibility, making them accessible across various platforms and tools.\\n\\nOther notable formats included _ReStructuredText_, _LaTeX_, _YAML_, and _blog posts_. These formats cater to specific needs, such as enhanced formatting capabilities, structured data representation, or providing more detailed explanations and context. Several unique and creative preferences also emerged, such as _Asciidoc_, _email_, and _PDFs_. This variety of preferences highlights that the needs of different projects and their users vary significantly.\\n\\n## Tools\\n\\nFollowing the format, we\u2019ve tried to collect tooling (if any) used by the audience to help with the changelog management. People mentioned various text editors, IDEs and of course, _chatgpt_/_ai_. But let\u2019s take a look at one specific tool worth sharing:\\n\\n### towncrier\\n\\nFrom [the project\u2019s homepage](https://towncrier.readthedocs.io/en/stable/index.html), _towncrier delivers the news which is convenient to those that hear it, not those that write it._ During a development, _\\"news fragments\\"_ (~text files) are created and when there is a new release, one can merge those together. Being user-centric and storing the fragments in git (and being able to review) makes it a really interesting choice worth exploring. Sadly, the pre-commit hooks can\u2019t remind you that you\u2019ve forgotten to add a new _news fragment_. Luckily, there is help in the form of a [Chronographer GitHub application](https://github.com/sanitizers/chronographer-github-app) created by [Sviatoslav Sydorenko (@webknjaz)](https://github.com/webknjaz) (who was by coincidence also present at the talk)\\n\\nQuite interestingly, 11% of the responders mentioned that no automation is used \u2013 there is an opportunity for improvement! (But to be fair, it can also mean that someone from the projects wants to prepare something really useful and do this all by hand.)\\n\\nThere were also some tools like _coffee_, _potato_, _postal pigeons_ or _beer_ that we weren\u2019t able to find documentation for. If you find these, let us know so we can add some links...;-)\\n\\nThere were also various git-based solutions suggested which leads us to the next question:\\n\\n## Automation based on commit messages\\n\\nThis is a tricky one, right? It might seem like an obvious choice of getting the input for our changelogs. But\u2026 yes, there is a \u201cbut\u201d. There are two main reasons why one wants to avoid using commit messages for this:\\n\\n- Commit messages are meant to be read by developers.\\n- Commit itself represents a change meaningful for developers, not users.\\n\\nBased on these observations, we came up with the following rules in our team (and talk attendees mentioned the same):\\n\\n- The content of the changelog should be created for users, not developers.\\n- Changelog should be created for the user-focused level of change => in our case, pull-request.\\n- Changelog should be created by the author of the change when the change is being developed.\\n\\nOf course, one can still use commits for this, but we don\u2019t think it\u2019s a good idea to have two goals for one text. If you really want to go this way, there is a [Conventional Commits project](https://www.conventionalcommits.org/). If nothing else, it can bring more attention to the commit messages and provide well-defined rules for the project contributors. (Talk attendees also mentioned git-cliff as a changelog generator for _Conventional Commits_.) You can also use this format independently to user-facing changelog. (Or, maybe as a base info for a human creating the changelog.) To mention also other responses, there were various git-log based solutions mentioned including the functionality provided directly by git-forges.\\n\\n![Conventional Commits: search for a user (nothing found)](./img/conventional_commits.png)\\n\\n## Packit blog-post generator\\n\\nAs a follow-up to the previous questions, we\u2019ve shown a solution we use in Packit. We researched and tried various solutions but this is what finally works for us:\\n\\n1. When submitting a pull-request, you put your changelog into a pull-request description.\\n\\n![Packit workflow 1: pull-request](./img/packit_workflow1.png)\\n\\n2. If you forget, a GitHub action will mark the PR red to remind you. (You can also put \u201cN/A\u201d if there is no user-facing change and the PR should be skipped for this check.)\\n\\n3. There is a GitHub action that we manually trigger when a new release should be prepared \u2013 as a result, new pull request is created with the aggregated changelog and version being updated.\\n   When this pull request is merged, the content of the changelog is also put into GitHub release description and from that taken when preparing downstream (i.e. Fedora) updates\\n\\n![Packit workflow 2: Manual triggering of Github workflow that prepares the changelog](./img/packit_workflow2.png)\\n![Packit workflow 3: Content of the pull request created by the workflow](./img/packit_workflow3.png)\\n![Packit workflow 4: Published release with the changelog from the pull request](./img/packit_workflow4.png)\\n\\n4. In Packit, most of our users do not install our packages manually but use our service. When doing a new deployment (by moving stable branches in our repositories), we collect the code snippets and prepare an update post that is published on our project page.\\n\\n![Packit workflow 5: Published blog post with changelog from the pull request](./img/packit_workflow5.png)\\n\\nImportant bit is that in both ways the changelog snippets are used, there is a review in place. So, you can still revisit the text, combine more entries together or remove if this is not relevant to the user after all.\\n\\nYou can, but don\u2019t need to use the same, but try to think about this, have a discussion within a team. The discussion itself can help you think more about your users.\\n\\n## Nice changelog examples\\n\\nThanks to our fellow attendees, we can share some examples to be inspired by:\\n\\n- https://github.com/signalapp/Signal-Desktop/releases\\n- https://github.com/packit/packit/blob/main/CHANGELOG.md (This wasn\u2019t us who suggested this...;)\\n- https://yarl.aio-libs.org/en/latest/changes/\\n- https://github.com/systemd/systemd/releases\\n- https://getgrav.org/#changelog (Nice visualisation!)\\n- https://cockpit-project.org/blog (Yes, blog posts can be used as well!)\\n- https://docs.djangoproject.com/en/5.0/releases/5.0/\\n- https://github.com/ClusterLabs/pcs/blob/main/CHANGELOG.md\\n- https://qgis.org/en/site/forusers/visualchangelogs.html (Visual changelogs!)\\n- https://github.com/ksh93/ksh/releases\\n- https://github.com/Hypfer/Valetudo/releases (screenshots, breaking changes, personal opinions,...)\\n- https://www.home-assistant.io/blog/categories/release-notes/ (screenshots, a lot of screenshots, deprecations, new plugins,..)\\n\\n## Space for improvement?\\n\\nIn addition to preferences, we also sought feedback on potential improvements for managing changelogs. The responses highlighted several key areas where the audience sees opportunities for enhancement:\\n\\n- **AI:** A significant number of responses emphasized the need for AI integration in changelog management, specifically using generative AI for writing changelogs was mentioned.\\n- **Standardization and consistency:** Several responses called for standardizing the format and content of changelogs. Consistency in how changelogs are written and maintained can improve readability and usability. Specific suggestions included using templates and setting ground rules, such as always including issue IDs in commits.\\n- **Automation and integration with development tools:** Improving the automation tools and Integrating changelog generation with existing development tools and CI/CD pipelines was another common suggestion. This could streamline the process, ensuring that changelogs are automatically updated and maintained as part of the development workflow.\\n- **Improving quality:** Improving the quality of changelog messages was a recurring theme. Responses suggested focusing on clear, concise, and meaningful wording and also highlighted the need for changelogs to be more user-oriented rather than developer-centric.\\n\\nSeveral responses addressed specific needs, such as differentiating upstream and downstream changelogs, supporting all CI systems, and referencing the tickets associated with changes. Additionally, there were responses emphasizing the importance of keeping changelogs simple and easy to understand.\\n\\n## Conclusion\\n\\nAnd now what?\\n\\nWhat you can do now? Improve changelog in your project. Get involved in the project you like and help with the changelogs. Read the changelos.\\n\\nWhat we can do together? Let\u2019s collaborate on the tools and share good practices!\\n\\nAnd what about the standardisation? Let\u2019s create a new standard! https://xkcd.com/927/\\n\\nWith that, let\u2019s quote a response from one of our attendees:\\n_\\"Many more people read the changelog than write it, so it\'s worth it to put in the effort.\\"_\\n\\n---\\n\\nThis post was also posted at [medium.com](https://medium.com/@laura.barcziova/do-you-like-your-changelogs-what-devconf-cz-attendees-think-771f51a4e3b0)."},{"id":"/devconf-2024","metadata":{"permalink":"/posts/devconf-2024","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/devconf-2024/index.md","source":"@site/posts/devconf-2024/index.md","title":"DevConf.CZ 2024 and week around for Packit","description":"The first part of June is usually quite busy for our team. Why? The last couple of years, this has been a time of DevConf.CZ conference. (The unpredictable January had been changed into a more pleasant June.)","date":"2024-07-01T00:00:00.000Z","formattedDate":"July 1, 2024","tags":[],"readingTime":4.605,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Lachman","email":"flachman@redhat.com","url":"https://github.com/lachmanfrantisek","imageURL":"https://github.com/lachmanfrantisek.png","key":"flachman"},{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"DevConf.CZ 2024 and week around for Packit","date":"2024-07-01T00:00:00.000Z","authors":["flachman","lbarczio"]},"prevItem":{"title":"Do you like your changelogs? What DevConf.CZ attendees think","permalink":"/posts/changelogs"},"nextItem":{"title":"Customize AWS cloud images with Image Builder and Packit","permalink":"/posts/aws-and-image-builder"}},"content":"The first part of June is usually quite busy for our team. Why? The last couple of years, this has been a time of [DevConf.CZ conference](https://devconf.cz/). (The unpredictable January had been changed into a more pleasant June.)\\nEven though the conference itself is important, it\u2019s used as an opportunity for various people from around the globe to come to Brno and thanks to that, a lot is happening also during the days around.\\nFor the Packit team, it\u2019s a nice opportunity to have the whole team together in one place \u2013 we can do some fun teambuilding (like canoeing this year) but also discuss any technical topics or meet our users and realise how are the real people behind all the nicknames. This time we also prepared something for them:\\n\\n![Packit team at DevConf.CZ](./img/team_at_devconf.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Packit workshop\\n\\nBefore DevConf, we recognized a unique opportunity: numerous users and potential users of Packit would be visiting Brno for the conference. Therefore, we decided to organise an in-person workshop with a main focus on our release automation. We had previously organised multiple online runs, for which you can find the materials [here](https://packit.dev/docs/workshops-materials), so we were mostly prepared. This initiative brought both Red Hatters and non-Red Hatters, resulting in a rich exchange of ideas and great feedback. In the end, the workshop served not only for learning about our release automation but also about the CI capabilities of Packit in upstream.\\n\\nDuring the workshop, several key areas of interest emerged:\\nBuilding in Sidetags - There was significant interest in building in sidetags. Participants provided valuable feedback on the workflow and configuration that Nikola Forro is currently developing, see [the GitHub issue](https://github.com/packit/packit/issues/1870). One of the discussion points was the automatic resolution of dependencies as the next step for the current static configuration.\\nCommon specfile manipulation tasks - Participants expressed a need for ways to handle common specfile manipulation tasks, which could be utilised in [Packit\'s actions](https://packit.dev/docs/configuration/actions) or for debugging purposes. Specific use cases included removing source/patch ranges for Copr builds, replacing sources within specfiles or getting and setting specfile versions. Our [specfile library](https://github.com/packit/specfile) already covers some of these, but besides that, also other alternative solutions were proposed, such as creating a CLI specifically for handling these tasks or adding Packit subcommands to facilitate these operations.\\n\\nBesides those, in relation to the release automation, some existing issues were brought up, such as the [one](https://github.com/packit/packit/issues/1724) about Packit to not create divergent branches when syncing release. Additionally, a bug/inconsistency was directly addressed and fixed during the workshop, see https://github.com/packit/packit/pull/2327 .\\n\\nOverall, the workshop was a success and we are happy for our great users for coming! The gained insights will definitely influence the ongoing development and improvement of Packit. We celebrated the successful workshop with a nice lunch together with the participants.\\n\\n## Packit members talks\\n\\nOf all the various proposals prepared by our team (and there as a lot!), 3 were accepted and we were able to show some interesting topics to the audience.\\n\\n![Laura and Franta presenting](./img/laura_franta_presenting.jpg)\\n\\nThe first session was an interactive on hold by Laura and Franti\u0161ek about changelogs \u2013 we used a Mentimer platform to be able to interact with the audience. We could not only collect information what people are interested to see in changelogs or what tooling do they use, but also showed charts from the research Laura made as part of here diploma thesis the last year. As part of the session, we were able to show the changelog automation we use in Packit. There is also [a blog post](https://packit.dev/posts/changelogs) covering the talk and all the interesting findings.\\n\\nFor the next session, Franti\u0161ek took a bunch of happy (of course..) Packit users and organised a user showcase. In just half an hour 8 people went to stage and provided an introduction to Packit, tmt, Testing Farm and showed 4 interesting usecases. Recording can be found [here](https://www.youtube.com/watch?v=7n8pypmrQh4). Interestingly, the two of the usecases overlayed \u2013 Cockpit has introduced their tests-cases into their dependencies to realise issues soon and one of such dependencies is Podman. Both Podman and Cockpit was presented on the stage.\\n\\nDuring the third session, Laura and Tom\xe1\u0161 showed our journey to team role rotation and how we do this these days. They used Mentimer as well as for the changelog one so it was a great fun. Missed it? No worries, there is no only a [recording](https://www.youtube.com/watch?v=y1t7Wd31bL8), but also [a blog-post serie](https://packit.dev/agile/weekly-roles) covering this topic. [The last part](https://medium.com/@laura.barcziova/role-rotation-tutorial-957ed3545ef2) helps you do the same, and as usual, we have this automated\u2026\\n\\n![Laura and Tom\xe1\u0161 presenting](./img/laura_and_tomas_presenting.jpg)\\n\\n## Talks related to Packit\\n\\nEven though some of our talk proposals were not accepted this year, our users represented us very well. In addition to the previously mentioned user showcase), there were two other dedicated talks by Packit users:\\n\\n- **Shifting Left in Podman with Copr, Packit, and TMT** - In this talk, Mohan Boddu described how they utilise Packit, Copr, and TMT to shift their testing setup and test early in the process. This talk also included a short live demo. (https://pretalx.com/devconf-cz-2024/talk/WVNJZS/)\\n- **Upstream Cross-Project Testing: Never Break Your API Consumers** - In this talk, Martin Pitt from the Cockpit team described the use case of reverse dependency testing (https://packit.dev/docs/cross-project-testing) using Packit. (https://pretalx.com/devconf-cz-2024/talk/KDZZES/)\\n\\nAdditionally, Siteshwar Vashisht presented about OpenScanHub, a service for static analysis of Linux distributions, where he also mentioned Packit and [our plans](https://github.com/packit/packit-service/issues/2107) for integration (https://pretalx.com/devconf-cz-2024/talk/7C38GJ/).\\n\\n---\\n\\nSo, that was it. A DevConf week we enjoy being part of same as enjoying it being over.\\n\\n![Team at a photo booth](./img/team_booth.jpg)"},{"id":"/aws-and-image-builder","metadata":{"permalink":"/posts/aws-and-image-builder","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/aws-and-image-builder/index.md","source":"@site/posts/aws-and-image-builder/index.md","title":"Customize AWS cloud images with Image Builder and Packit","description":"Have you ever wanted to bring your pull request changes in a cloud image easily?","date":"2024-01-22T00:00:00.000Z","formattedDate":"January 22, 2024","tags":[{"label":"image-builder","permalink":"/posts/tags/image-builder"}],"readingTime":5.5,"hasTruncateMarker":true,"authors":[{"name":"Maja Massarini","email":"mmassari@redhat.com","url":"https://github.com/majamassarini","imageURL":"https://github.com/majamassarini.png","key":"mmassari"}],"frontMatter":{"title":"Customize AWS cloud images with Image Builder and Packit","date":"2024-01-22T00:00:00.000Z","authors":"mmassari","tags":["image-builder"]},"prevItem":{"title":"DevConf.CZ 2024 and week around for Packit","permalink":"/posts/devconf-2024"},"nextItem":{"title":"Introduction to specfile library","permalink":"/posts/specfile-introduction"}},"content":"Have you ever wanted to bring your pull request changes in a cloud image easily?\\nCurious about how easy it can be? With Packit, it can be just about commenting on your pull request with `/packit vm-image-build`.\\n\\nWith the above command, Packit automates all the manual steps needed to create an\\nRPM package with your pull request changes and asks the Image Builder to install it\\ninside a brand new cloud image.\\nLet\'s have a look at the prerequisites for this.\\n\\n\x3c!--truncate--\x3e\\n\\n# Join the Red Hat Developer Program\\n\\nIf you don\'t already have a business account you can create a\\n_Red Hat Developer account_ at no cost [here](https://developers.redhat.com/about).\\n\\nYou need a subscription in order to use the\\n[Image Builder service](https://console.redhat.com/insights/image-builder)\\nand launch the builded images in the [AWS management console](https://aws.amazon.com/console/).\\n\\n# Prepare to upload AWS AMI images\\n\\nBefore uploading an AWS AMI image, you must configure the AWS system for receiving them.\\n\\n### Prerequisites\\n\\n- You must have an Access Key ID configured in the [AWS IAM account manager](https://aws.amazon.com/iam/).\\n- You must have a writable [S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) prepared.\\n\\n### Procedure\\n\\nFollow [these steps](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/composing_a_customized_rhel_system_image/creating-cloud-images-with-composer_composing-a-customized-rhel-system-image#preparing-for-uploading-aws-ami-images_creating-cloud-images-with-composer)\\nto satisfy the above prerequisites.\\n\\n# The manual steps\\n\\nAre you wondering what are the manual steps for bringing your pull request changes\\nin a cloud image and why you should automate them?\\n\\nThere could be many ways to achieve this goal but let\'s see together the closest to our\\nautomated solution. Below you can find a summary of all the needed manual steps;\\nI am quite sure after reading them, you will want to automate them with Packit!\\n\\n- Build an RPM package with your pull request changes through **COPR**, go to https://copr.fedorainfracloud.org\\n\\n  1. Install `copr-cli`.\\n  2. Create your account and service token.\\n  3. Add your token to `~/.config/copr.\\n  4. Create a new COPR project.\\n  5. Start a build with your local pull request changes using `copr-cli`.\\n  6. **WAIT for the build to finish**.\\n\\n- Create a new cloud image through the **Image Builder console**, go to https://console.redhat.com/insights/image-builder\\n\\n  7. Login with your _Red Hat developer_ account.\\n  8. Click on the `Create Image` button, choose _AWS image_ type and follow the wizard.\\n  9. **WAIT for the build to finish**.\\n  10. Open the `Launch` link for the builded image.\\n\\n- Launch and access the AWS image through the **AWS management console**, go to https://aws.amazon.com/console/\\n\\n  11. The previous link will open an AWS console tab with the\\n      _Launch an Instance_ wizard preset to use the builded image.\\n      You need to login into the _AWS management console_ using an _AWS Account ID_\\n      allowed to access the _AMI Image_ you just created.\\n  12. Select a **Key pair**, or create one if you don\'t have it already,\\n      to be able to ssh the image later.\\n  13. Click on `Launch Instance`\\n  14. Connect to instance using an ssh client\\n  15. Add the previously created COPR repo to the list of available dnf repositories.\\n  16. Install the package you have created at step number 4.\\n  17. Now you are ready to test your code in a real cloud image.\\n\\nFor every new pull request you want to test directly in a cloud image you have to repeat\\nsteps 4-16 or automate them through Packit!\\n\\n# Automate the steps\\n\\n## Install Packit\\n\\nInstalling Packit is pretty straightforward.\\n\\n1.  Create a valid [Fedora Account System (FAS)](https://fedoraproject.org/wiki/Account_System)\\n    account (if you don\'t already have one).\\n    Why do you need it? After these few steps you will start building (and potentially shipping)\\n    Fedora packages through the [COPR service](https://copr.fedorainfracloud.org/) and we need you to agree with the Fedora license.\\n2.  Install our GitHub application on [GitHub Marketplace](https://github.com/marketplace/packit-as-a-service),\\n    or [configure a webhook](https://packit.dev/docs/guide/#how-to-set-up-packit-on-gitlab) on GitLab\\n    (depending on where your project lives).\\n3.  Make Packit [approve your FAS username](https://packit.dev/docs/guide/#2-approval);\\n    on Github the approval process is automated and for Gitlab you have to contact us.\\n\\nNow you are ready to automate the process as described below.\\n\\n## Setup Packit\\n\\nCreate a `.packit.yaml` configuration file in your pull request.\\n\\nBut just the first time! After your pull request has been merged, Packit will take the `.packit.yaml` file from the target _main branch_.\\n\\nThe configuration file will look like the following:\\n\\n```\\n---\\n\\njobs:\\n- job: copr_build\\n  trigger: pull_request\\n  targets:\\n  - fedora-all\\n\\n- job: vm_image_build\\n  trigger: pull_request\\n  image_request:\\n    architecture: x86_64\\n    image_type: aws\\n    upload_request:\\n      type: aws\\n      options:\\n        share_with_accounts:\\n        - < shared-aws-account-id >\\n  image_distribution: fedora-39\\n  copr_chroot: fedora-39-x86_64\\n  image_customizations:\\n    packages: [hello-world]\\n```\\n\\n### copr_build job\\n\\nThe first job tells Packit service to build an RPM package, for the Fedora release you want,\\nin this example all the active fedora releases, and to add your pull request changes to the package.\\n\\nTo further customize the COPR builds made by Packit you may want to give a look at this\\n[guide](https://packit.dev/docs/configuration/upstream/copr_build).\\n\\n### vm_image_build job\\n\\nThe second job tells Packit how to configure the Builder Image service.\\n\\nThe first two lines of this job are still meant for Packit;\\nthey allow Packit to react to your pull request comment `/packit vm-image-build`.\\nPackit does not build a VM image automatically, as it does when it builds a COPR package,\\nto save you from no wanted costs.\\n\\n```\\n- job: vm_image_build\\ntrigger: pull_request\\n```\\n\\nThe other lines are meant to customize the Image Builder behaviour.\\n\\nYou are asking to build an _AWS_ image, with a _fedora-39_ distribution,\\nfor the _x86_64_ architecture and you want to share it with the listed\\n_AWS Account IDs_.\\n\\n```\\nimage_request:\\n  architecture: x86_64\\n  image_type: aws\\n  upload_request:\\n    type: aws\\n    options:\\n      share_with_accounts:\\n      - < shared-aws-account-id >\\nimage_distribution: fedora-39\\n```\\n\\nYou don\'t want to manually install the COPR package into the image,\\nfor this reason you ask the Image Builder to install it (_hello-world_).\\n\\nYou tell Image Builder to take it from the COPR chroot _fedora-39-x86_64_,\\nand you don\'t need to create or specify a COPR project because it has\\nbeen automatically created by Packit for you.\\n\\n```\\ncopr_chroot: fedora-39-x86_64\\nimage_customizations:\\n  packages: [hello-world]\\n```\\n\\n## Create, comment and test a pull request!\\n\\nCreate a pull request, mine will show you the **world** word in green \ud83c\udf3f.\\n\\nYou are ready to go, just comment your pull request with\\n\\n`/packit vm-image-build`\\n\\nand the image will be built and customized for you.\\n\\nLook for the check named **vm-image-build-fedora-39-x86_64**\\nand wait for it to finish.\\n\\n![Wait for check vm-image-build-fedora-39-x86_64 to finish](images/checks_vm_image_build.png)\\n\\nOpen its details and you will find the link\\nto the AWS image.\\n\\n![The check details have a link to the AWS image](images/link_to_aws_image.png)\\n\\nOpen the AWS link (you need to be already logged in) and\\nsee the details of your image ready to be launched.\\n\\n![The AWS image details](images/ami-link.png)\\n\\nLaunch your image instance and connect to it.\\n\\n![Connect to instance details](images/connect-to-instance.png)\\n\\nTest it!\\n\\n![Test it!](images/hello-world.png)"},{"id":"/specfile-introduction","metadata":{"permalink":"/posts/specfile-introduction","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/specfile-introduction/index.md","source":"@site/posts/specfile-introduction/index.md","title":"Introduction to specfile library","description":"Have you ever wanted to make changes in an RPM spec file programmatically? specfile library","date":"2024-01-12T00:00:00.000Z","formattedDate":"January 12, 2024","tags":[{"label":"specfile","permalink":"/posts/tags/specfile"}],"readingTime":3.875,"hasTruncateMarker":true,"authors":[{"name":"Nikola Forr\xf3","email":"nforro@redhat.com","url":"https://github.com/nforro","imageURL":"https://github.com/nforro.png","key":"nforro"}],"frontMatter":{"title":"Introduction to specfile library","date":"2024-01-12T00:00:00.000Z","authors":"nforro","tags":["specfile"]},"prevItem":{"title":"Customize AWS cloud images with Image Builder and Packit","permalink":"/posts/aws-and-image-builder"},"nextItem":{"title":"Experiences using Packit for a Rust executable Project","permalink":"/posts/experiences_with_rust"}},"content":"Have you ever wanted to make changes in an RPM spec file programmatically? _specfile_ library\\nhas been created for that very purpose. It is a pure Python library that allows you to conveniently\\nedit different parts of a spec file while doing its best to keep the resulting changeset minimal\\n(no unnecessary whitespace changes etc.).\\n\\n\x3c!--truncate--\x3e\\n\\n## Installation\\n\\nThe library is packaged for Fedora, EPEL 9 and EPEL 8 and you can simply install it with dnf:\\n\\n```bash\\ndnf install python3-specfile\\n```\\n\\nOn other systems, you can use pip (just note that it requires RPM Python bindings to be installed):\\n\\n```bash\\npip install specfile\\n```\\n\\n## Usage\\n\\nLet\'s have a look at a few simple examples of how to use the library.\\n\\n### Bumping release\\n\\nTo bump release and add a new changelog entry, we could use the following code:\\n\\n```python\\nfrom specfile import Specfile\\n\\nwith Specfile(\\"example.spec\\") as spec:\\n    spec.release = str(int(spec.expanded_release) + 1)\\n    spec.add_changelog_entry(\\"- Bumped release for test purposes\\")\\n```\\n\\nLet\'s take a look at what happens here:\\n\\nWe instantiate `Specfile` class with a path to our spec file and use it as a context manager\\nto automatically save all changes upon exiting the context.\\n\\nWe then use `expanded_release` property to get the current value of `Release` tag after macro expansion.\\nWe assume it is numeric, so we simply convert it to integer, add 1, convert the result back to string\\nand assign the new value to `release` property.\\n\\n:::tip\\n\\nNote that `release`/`expanded_release` properties exclude dist tag (usually `%{?dist}`) - for convenience,\\nit is ignored when reading and preserved unmodified when writing. If that\'s not what you want, you can use\\n`raw_release`/`expanded_raw_release` properties instead.\\n\\n:::\\n\\nFinally, we add a new changelog entry. We don\'t specify any other arguments but content,\\nso the author is determined automatically using the same procedure as `rpmdev-packager` uses\\nand date is set to current day.\\n\\n### Switching to `%autochangelog`\\n\\nTo make a switch from traditional changelog to `%autochangelog`, we could do the following:\\n\\n```python\\nimport pathlib\\nfrom specfile import Specfile\\n\\nspec = Specfile(\\"example.spec\\", autosave=True)\\n\\nwith spec.sections() as sections:\\n    entries = sections.changelog[:]\\n    sections.changelog[:] = [\\"%autochangelog\\"]\\n\\npathlib.Path(\\"changelog\\").write_text(\\"\\\\n\\".join(entries) + \\"\\\\n\\")\\n```\\n\\nLet\'s take a look at what happens here:\\n\\nWe instantiate `Specfile` class with a path to our spec file and we also set `autosave` argument\\nthat ensures that any changes are saved automatically as soon as possible.\\n\\n_specfile_ heavily relies on context managers. Here we are using `sections()` method that returns\\na context manager that we can use to manipulate spec file sections. Upon exiting the context,\\nany modifications done are propagated to the internal representation stored in our `Specfile` instance,\\nand since `autosave` is set, they are immediately saved to the spec file as well.\\n\\nFirst, we store a copy of the content of the `%changelog` section. The content is represented\\nas a list of lines.\\n\\nThen we replace the content with a single line - \\"%autochangelog\\".\\n\\nFinally, we save the stored content into a \\"changelog\\" file.\\n\\n### Iterating through tags\\n\\nContexts can be nested. Here is a code that iterates through all _package_ sections\\n(including the first, implicitly named one; also known as _preamble_) and prints expanded value\\nof all `Requires` tags:\\n\\n```python\\nspec = Specfile(\\"example.spec\\")\\n\\nwith spec.sections() as sections:\\n    for section in sections:\\n        # normalized name of a section is lowercased\\n        if section.normalized_name != \\"package\\":\\n            continue\\n        with spec.tags(section) as tags:\\n            for tag in tags:\\n                # normalized name of a tag is capitalized\\n                if tag.normalized_name != \\"Requires\\":\\n                    continue\\n                print(f\\"Section: {section.id}, Tag: {tag.name}, Value: {tag.expanded_value}\\")\\n```\\n\\nLet\'s take a look at what happens here:\\n\\nWe instantiate `Specfile` class with a path to our spec file. This time we don\'t set `autosave` because\\nwe are not doing any modifications (though we could still save any changes explicitly using `save()` method).\\n\\nThen we use `sections()` context manager and iterate through sections; we skip sections not called \\"package\\"\\n(the initial _%_ is ommited for convenience).\\n\\nAfter that we use `tags()` context manager and pass the current section as an argument. This allows us\\nto iterate through tags in the current section. Without any argument, we would get a list of tags in _preamble_,\\nthe very first section in a spec file. We skip tags not called \\"Requires\\" and finally print the values\\nof `Requires` tags after macro expansion. We also print tag names (not normalized) and section IDs - those are\\nsection names followed by options, e.g. \\"package -n alternative-name-for-example\\".\\n\\n## More info and links\\n\\nAre you interested in more details, trying the library out or even contributing? You can find _specfile_ source code on [GitHub](https://github.com/packit/specfile).\\nSee the [README](https://github.com/packit/specfile/blob/main/README.md) for more tips and usage examples.\\nYou can also check out the [API reference](https://packit.dev/specfile/api/specfile)."},{"id":"/experiences_with_rust","metadata":{"permalink":"/posts/experiences_with_rust","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/experiences_with_rust/index.md","source":"@site/posts/experiences_with_rust/index.md","title":"Experiences using Packit for a Rust executable Project","description":"\\"How absurdly simple!\\" I cried.","date":"2023-10-10T00:00:00.000Z","formattedDate":"October 10, 2023","tags":[],"readingTime":4.55,"hasTruncateMarker":true,"authors":[{"name":"mulhern","email":"amulhern@redhat.com","url":"https://github.com/mulkieran","imageURL":"https://github.com/mulkieran.png","key":"amulhern"}],"frontMatter":{"title":"Experiences using Packit for a Rust executable Project","date":"2023-10-10T00:00:00.000Z","authors":"amulhern"},"prevItem":{"title":"Introduction to specfile library","permalink":"/posts/specfile-introduction"},"nextItem":{"title":"Call for volunteers: help to test us the release syncing using staging instance","permalink":"/posts/verify-sync-release-volunteers"}},"content":"\\"How absurdly simple!\\" I cried.\\n\\n\\"Quite so!\\" said he, a little nettled. \\"Every problem becomes very childish when once it is explained to you.\\"\\n\\n- Arthur Conan Doyle, \\"The Adventure of the Dancing Men\\"\\n\\nWe have planned for a while to use Packit to generate packages on Copr\\non demand for our somewhat complicated Rust executable, stratisd. It\\nlooked like this was going to be challenging, and in a sense it was,\\nbut once the task was completed, it turned out to have been pretty\\nstraightforward.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe two primary parts of our Stratis project are stratisd, a big,\\nfairly complicated Rust project which generates multiple distinct Rust\\nexecutables and also installs scripts and configuration files and so\\nforth, and stratis-cli, a relatively simple Python project which\\ncommunicates with stratisd over the D-Bus. Our main purpose in getting\\non-demand packages of both these projects was not to test the\\npackaging, although that is a nice thing to do, but to have a\\nconvenient way to do functional testing of the project, frequently\\ngenerating a stratisd package from one stratisd pull request and a\\nstratis-cli package from a matching stratis-cli pull request.\\n\\nThe Packit team had already obliged us by preparing a Packit\\nconfiguration file for our stratis-cli repo, we thus merely had to\\nfigure out how to do the same for stratisd. We already had some\\nscripts which we could run by hand to make the SRPMs for both projects\\nso that we could manually send them to Copr so, as it turned out, all\\nwe really needed to do was figure out how to write a Packit\\nconfiguration file for stratisd so that Packit would do these tasks\\nfor us. There follows a quick summary of the decisions we made to\\nsolve this problem. Note that our whole and only task was to figure\\nout how to create the source artifacts and edit the spec file and\\nto provide these in such a way that Packit could take over from there,\\nbuild the SRPM, and carry on to request the Copr builds which provide\\nthe binary packages for testing.\\n\\nA Packit configuration is used to set certain configuration _options_,\\nto override certain _actions_, or to define certain _hooks_. The\\navailable actions and hooks are specific to a given job or set of jobs.\\nIf an action is not\\noverridden in a project\'s Packit configuration, Packit will execute\\nits own default action. For example, Packit\'s default for the\\ncreate-archive action is just git-archive. Any action can be overridden by\\nspecifying a sequence of commands. For the stratisd Packit\\nconfiguration, we found that we had to override the majority of the\\nactions.\\n\\nNote that the [stratisd Packit configuration file] is quite\\nsimple as most of the action is in the `create_archive.py` script. Some\\nthings to make note of are the following:\\n\\n1. `merge_pr_in_ci` is set to false. This prevents Packit from\\n   automatically merging our pull request into its target branch. It has\\n   always been our custom to build from the unmerged branch when testing\\n   PRs and we preferred to do the same in our Packit configuration.\\n\\n2. `fix-spec-file` contains a dummy echo action. This is how we override\\n   Packit\'s default spec file editing action. Instead of specifying an\\n   override for this action, we use [our own script], `create_artifacts.py`, to\\n   edit the spec file as well as to generate the three source artifacts that we\\n   use in this package in the `create-archive` action.\\n\\n3. `create_artifacts.py` uses the Packit team\'s [specfile package] to\\n   edit the stratisd specfile. We needed just two modifications: 1. To overwrite the Version field with our preferred version string. 2. To overwrite the Source entries with the names and paths of the\\n   source artifacts.\\n   This requires just one [very simple function] using specfile.\\n\\n4. In the Packit configuration, the `update_release` field is set to\\n   false. We prefer to update the Version field in the spec file using\\n   our `create_archive.py` script, so that the updated version is\\n   recognized as a pre-release version of our current version by\\n   `rpmdev-vercmp`.\\n\\n5. We modified our [stratis-cli Packit configuration file] to work the\\n   same way as our stratisd one.\\n\\nIn summary, we got the ability to take advantage of the Packit\\ninfrastructure for Copr builds by figuring out how to satisfy its\\nrequirements for the source artifacts and the specfile. Note that our\\ndecisions about how to do this were made based on the scripts we had\\nalready written to generate the SRPMs and the particular requirements\\nof our project. For example, our release scripts are written in\\nPython, and we stuck with that choice. That turned out to be handy,\\nbecause that allowed us to use the specfile project in our scripts to\\nedit the spec file. But it would have been possible to write the\\nartifact creation script in Rust using the cargo xtask pattern just as\\nwell, and to solve the spec file rewriting problem using, e.g., sed.\\nWe overwrote the Version field in the spec file, rather than allowing\\nPackit to autobump our release number, because our development process\\nrequires us to bump the version in the Cargo.toml (for Rust) and\\nsetup.cfg (for Python) files to the version we will release as soon as\\nwe begin development. These choices and their implementations could be\\nmade differently to support a project with different constraints and\\ndifferent existing infrastructure.\\n\\n[stratisd Packit configuration file]: https://github.com/stratis-storage/stratisd/blob/master/.packit.yaml\\n[specfile package]: https://github.com/packit/specfile\\n[very simple function]: https://github.com/stratis-storage/ci/blob/master/release_management/_utils.py#L80\\n[stratis-cli Packit configuration file]: https://github.com/stratis-storage/stratis-cli/blob/master/.packit.yaml\\n[our own script]: https://github.com/stratis-storage/ci/blob/master/release_management/create_artifacts.py"},{"id":"/verify-sync-release-volunteers","metadata":{"permalink":"/posts/verify-sync-release-volunteers","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/verify-sync-release-volunteers/index.md","source":"@site/posts/verify-sync-release-volunteers/index.md","title":"Call for volunteers: help to test us the release syncing using staging instance","description":"In the upcoming months, we plan to migrate our service to a new cluster. However, this may affect propose_downstream","date":"2023-09-05T12:46:18.000Z","formattedDate":"September 5, 2023","tags":[{"label":"downstream","permalink":"/posts/tags/downstream"},{"label":"pull-from-upstream","permalink":"/posts/tags/pull-from-upstream"},{"label":"propose-downstream","permalink":"/posts/tags/propose-downstream"},{"label":"staging","permalink":"/posts/tags/staging"}],"readingTime":1.84,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"Call for volunteers: help to test us the release syncing using staging instance","date":"2023-09-05T12:46:18.000Z","authors":"lbarczio","tags":["downstream","pull-from-upstream","propose-downstream","staging"]},"prevItem":{"title":"Experiences using Packit for a Rust executable Project","permalink":"/posts/experiences_with_rust"},"nextItem":{"title":"(Tests) job triggering improvements","permalink":"/posts/manual-triggering"}},"content":"In the upcoming months, we plan to migrate our service to a new cluster. However, this may affect `propose_downstream`\\nand `pull_from_upstream` jobs due to the new firewall rules. The problematic aspects could be:\\n\\n- commands you run in your `actions` during syncing the release involving interactions with external servers\\n- downloading your sources from various hosting services (crates.io, npm, gems, etc.)\\n\\nTo smoothen this transition, we kindly encourage you to enable one of these jobs on our already migrated staging instance.\\nThis recommendation is particularly important if you belong to one of the groups affected by the two previous points.\\nThis proactive step will help us identify and address any issues promptly.\\n\\n\x3c!--truncate--\x3e\\n\\nBoth instances can be run at the same time and the behaviour can be configured via the `packit_instances` configuration key,\\nwhich is by default set to `[\\"prod\\"]`. Picking just one instance is required only for `koji_build` and `bodhi_update` jobs since\\nboth instances work with the production instances of Fedora systems. To avoid too much noise in your dist-git PRs, you\\nmay enable the `pull_from_upstream`/`propose_downstream` job for only one target, resulting in only one additional PR created.\\n\\nHere\'s how you can enable one of the jobs on the staging instance:\\n\\n- `pull-from-upstream`:\\n  The only thing needed is to duplicate the job in your Packit config using `packit_instances` configuration option. Example:\\n\\n```yaml\\n- job: pull_from_upstream\\n  trigger: release\\n  packit_instances: [\\"stg\\"]\\n  dist_git_branches:\\n    - fedora-rawhide\\n```\\n\\n- `propose-downstream`:\\n  For this job, you first need to [enable our staging Github app](/docs/guide/#staging-instance)\\n  (you should be already automatically approved if you had been previously approved for production instance).\\n  After that, similarly to `pull-from-upstream`, you only need to duplicate the job in your Packit config using `packit_instances`. Example:\\n\\n```yaml\\n- job: propose_downstream\\n  trigger: release\\n  packit_instances: [\\"stg\\"]\\n  dist_git_branches:\\n    - fedora-rawhide\\n```\\n\\n:::info\\n\\nWhen merging the PRs created by Packit, please don\'t forget to merge the PRs created by the production instance\\nif you have a follow-up `koji_build` job enabled to ensure your builds will not be skipped\\n(or you can allow builds for staging instance as well, see [allowed_pr_authors](/docs/configuration/downstream/koji_build#optional-parameters))).\\n\\n:::\\n\\nWe would be happy if you could then report any problems to [us](#contact). We appreciate your collaboration in ensuring a seamless migration. Your Packit team!"},{"id":"/manual-triggering","metadata":{"permalink":"/posts/manual-triggering","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/manual-triggering/index.md","source":"@site/posts/manual-triggering/index.md","title":"(Tests) job triggering improvements","description":"Recently, we received multiple contributions from the Strimzi team, specifically Jakub Stejskal","date":"2023-07-11T06:46:18.000Z","formattedDate":"July 11, 2023","tags":[{"label":"configuration","permalink":"/posts/tags/configuration"}],"readingTime":2.88,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"(Tests) job triggering improvements","date":"2023-07-11T06:46:18.000Z","authors":"lbarczio","tags":["configuration"]},"prevItem":{"title":"Call for volunteers: help to test us the release syncing using staging instance","permalink":"/posts/verify-sync-release-volunteers"},"nextItem":{"title":"Introducing monorepository support","permalink":"/posts/monorepos"}},"content":"Recently, we received multiple contributions from the Strimzi team, specifically [Jakub Stejskal](https://github.com/Frawless)\\nand [David Kornel](https://github.com/kornys),\\nfor the functionality of manual triggering of jobs and other related improvements, mostly focusing on the testing\\nUX. We are very happy about these and would like to showcase the results of their awesome contributions.\\n\\n\x3c!--truncate--\x3e\\n\\nStrimzi team wanted to onboard Packit to ease their testing in pull requests, but they have had multiple test suites that could run\\nfor a long time, and they did not want to run all of the test suites for each new commit in a pull request.\\n\\nLet\'s now look together at what they implemented to solve their usecase.\\n\\n## Manual-only triggering of jobs\\n\\nFirstly, a new configuration option [`manual_trigger`](/docs/configuration/jobs#manual_trigger) was introduced.\\nWith this new configuration option, users can enable triggering Packit jobs only manually and avoid specified jobs being\\nautomatically triggered when, e.g., a new commit arrives to a pull request.\\nThe only thing needed to make this work is to add `manual_trigger: true` to the job\'s definition:\\n\\n```yaml\\n- job: tests\\n  trigger: pull_request\\n  targets:\\n    - centos-stream-9-x86_64\\n  skip_build: true\\n  manual_trigger: true\\n```\\n\\nThis new configuration option allows saving resources and running builds or tests, e.g. only when the pull request is ready for\\ntesting and can be especially useful for projects having huge test plans.\\n\\n## Triggering test jobs based on labels and identifiers\\n\\nThe above solution is very easy to use; however, there might be use cases where the users don\u2019t want to trigger all the jobs.\\nA particular example could be running a smoke test job before running the sanity one.\\n\\nTo solve this, for the test jobs, Jakub and David introduced 2 new ways to trigger a specific job.\\n\\nThe first one is to trigger the job based on the configured [`identifier`](/docs/configuration/upstream/tests#optional-parameters). To trigger a test job with `identifier: regression-operators` in the\\njob configuration, the Packit comment command will be:\\n\\n    /packit test --identifier regression-operators\\n\\nThat command will execute the job with this specific identifier, nothing else.\\n\\nAnd what if someone wants to execute more than one job? Users can easily use multiple identifiers in a comma-separated list:\\n\\n    /packit test --identifier regression-operators,regression-components\\n\\nBut specifying a longer list of identifiers every time might get a little bit annoying,\\nespecially when the identifiers are usually used together repeatedly.\\n\\nTo improve the UX, there was introduced [`labels`](/docs/configuration/upstream/tests#optional-parameters) configuration field that can group together multiple jobs.\\nEach job can contain a list of `labels` in their definition:\\n\\n```yaml\\n- job: tests\\n  trigger: pull_request\\n  identifier: regression-operators\\n  targets:\\n    - centos-stream-9-x86_64\\n  skip_build: true\\n  manual_trigger: true\\n  labels:\\n    - regression\\n    - sanity\\n\\n- job: tests\\n  trigger: pull_request\\n  identifier: regression-components\\n  targets:\\n    - centos-stream-9-x86_64\\n  skip_build: true\\n  manual_trigger: true\\n  labels:\\n    - regression\\n\\n- job: tests\\n  trigger: pull_request\\n  identifier: acceptance-operators\\n  targets:\\n    - centos-stream-9-x86_64\\n  skip_build: true\\n  manual_trigger: true\\n  labels:\\n    - acceptance\\n    - sanity\\n```\\n\\nWith this, one can comment\\n\\n    /packit test --labels regression\\n\\nand the comment will trigger all jobs that contain `regression` in the list of labels in the job configuration.\\nIt is again also possible to specify a comma-separated list of labels:\\n\\n    /packit test --labels regression,sanity\\n\\n## Conclusion\\n\\nAs already mentioned, these improvements were made as contributions from outside of the Packit team, and it was such\\na nice experience to collaborate with someone new! And this brings us to a reminder:\\nAnyone can contribute! So if you are missing some features, feel free to open a pull request, and we will gladly help\\nyou so that your feature can land in our production!"},{"id":"/monorepos","metadata":{"permalink":"/posts/monorepos","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/monorepos/index.md","source":"@site/posts/monorepos/index.md","title":"Introducing monorepository support","description":"We are very happy to announce a major enhancement to Packit! We have now added support for","date":"2023-07-04T07:20:06.000Z","formattedDate":"July 4, 2023","tags":[{"label":"monorepos","permalink":"/posts/tags/monorepos"}],"readingTime":2.27,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"Introducing monorepository support","date":"2023-07-04T07:20:06.000Z","authors":"lbarczio","tags":["monorepos"]},"prevItem":{"title":"(Tests) job triggering improvements","permalink":"/posts/manual-triggering"},"nextItem":{"title":"Handling of Release field in propose_downstream job","permalink":"/posts/release-field-handling"}},"content":"We are very happy to announce a major enhancement to Packit! We have now added support for\\nmonorepositories, enabling the integration of upstream repositories containing multiple downstream packages.\\nIf you have a repository in the monorepo format, Packit can now help you automate the integration to downstream\\ndistributions both from CLI and as a service.\\n\\n\x3c!--truncate--\x3e\\n\\n## Configuration\\n\\nLet\'s take a look at how a monorepository should be configured so that Packit can automate the process!\\n\\nThe main addition to the Packit configuration file in the context of monorepositories are the keys\\n[`packages`](/docs/configuration#packages) and [`paths`](/docs/configuration#paths).\\n\\n`packages` holds a dictionary of `{package_name: package_configuration}` where package configuration can contain any keys\\nthat were previously used as top-level keys for the standard (single) package configuration. The `paths` can be defined\\nin each `package_configuration` and should hold a list of paths that should be considered for the particular package.\\n\\nThe `packages` section in the configuration can then look like this:\\n\\n```yaml\\npackages:\\n  python-copr:\\n    downstream_package_name: python-copr\\n    upstream_package_name: copr\\n    paths:\\n      - ./python\\n    specfile_path: python-copr.spec\\n    files_to_sync:\\n      - python-copr.spec\\n\\n  copr-cli:\\n    downstream_package_name: copr-cli\\n    upstream_package_name: copr-cli\\n    paths:\\n      - ./cli\\n    specfile_path: copr-cli.spec\\n    files_to_sync:\\n      - copr-cli.spec\\n\\n  copr-backend:\\n    downstream_package_name: copr-backend\\n    upstream_package_name: copr-backend\\n    paths:\\n      - ./backend\\n    specfile_path: copr-backend.spec\\n    files_to_sync:\\n      - copr-backend.spec\\n```\\n\\nThat was the configuration of the packages and their locations in general, but how should these values then be utilised?\\n\\n### Packit Service support\\n\\nFor the Packit Service jobs, there is one more addition to the configuration: the [`packages`](/docs/configuration/jobs#packages) key again,\\nthat can be used in each job and tells which packages should be considered for that particular job.\\n\\nAs a result, the repository in monorepo format could have jobs in the Packit configuration configured like this:\\n\\n```yaml\\njobs:\\n  - job: copr_build\\n    packages:\\n      - copr-backend\\n      - copr-cli\\n    trigger: pull_request\\n    targets:\\n      - fedora-all-x86_64\\n    preserve_project: True\\n\\n  - job: copr_build\\n    packages:\\n      - python-copr\\n    trigger: pull_request\\n    targets:\\n      - fedora-all-x86_64\\n      - fedora-all-aarch64\\n    manual_trigger: True\\n```\\n\\nAnd the commit statuses in GitHub/GitLab will also include the name of the package:\\n![Monorepo statuses](img/statuses.png)\\n\\n### CLI support\\n\\nAs for the CLI, you can now for each command specify the `-p` or `--packages` argument followed by the package name\\nand the scope of the command will take into consideration only the specified packages:\\n\\n    packit build in-mock --package my-package-1 --package my-package-2\\n\\n## Wrapping up\\n\\nIf you have a repository in the monorepo format, give it a try now and share your feedback with us!\\n\\nGoing forward, we are about to implement the [possibility of defining dependencies between jobs](https://github.com/packit/packit-service/issues/2105),\\nallowing for even greater flexibility and control of monorepository jobs.\\n\\nWe are also happy to help with any contributions from the community to help us expand and refine the support with additional functionalities,\\nsuch as [being able to define paths the service should react to](https://github.com/packit/packit-service/issues/2006)\\nor [templating features](https://github.com/packit/packit/issues/1925)."},{"id":"/release-field-handling","metadata":{"permalink":"/posts/release-field-handling","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/release-field-handling/index.md","source":"@site/posts/release-field-handling/index.md","title":"Handling of Release field in propose_downstream job","description":"We have recently made some fixes to the process of handling the Release field regarding the %autorelease macro, so let\'s take that opportunity to explain how it works.","date":"2023-05-23T12:00:00.000Z","formattedDate":"May 23, 2023","tags":[{"label":"specfile","permalink":"/posts/tags/specfile"},{"label":"propose-downstream","permalink":"/posts/tags/propose-downstream"}],"readingTime":1.74,"hasTruncateMarker":true,"authors":[{"name":"Nikola Forr\xf3","email":"nforro@redhat.com","url":"https://github.com/nforro","imageURL":"https://github.com/nforro.png","key":"nforro"}],"frontMatter":{"title":"Handling of Release field in propose_downstream job","date":"2023-05-23T12:00:00.000Z","authors":"nforro","tags":["specfile","propose-downstream"]},"prevItem":{"title":"Introducing monorepository support","permalink":"/posts/monorepos"},"nextItem":{"title":"Packit\'s pre-commit hooks","permalink":"/posts/pre-commit-hooks"}},"content":"We have recently made some fixes to the process of handling the `Release` field regarding the `%autorelease` macro, so let\'s take that opportunity to explain how it works.\\n\\n\x3c!--truncate--\x3e\\n\\nYou can maintain the `Release` field manually, you can use a dummy value or you can take advantage of the `%autorelease` macro.\\n\\n### Maintaining `Release` manually\\n\\nYou can maintain the `Release` field manually as you would do downstream, for example you could bump the value every time you make changes to the spec file and reset it to _1_ when you release a new version.\\n\\nPackit will not touch the value unless it detects a change in `Version` and no change in `Release`. In such case it will reset `Release` to _1_ in the dist-git spec file, to ensure that the resulting NVR is not higher than any existing NVR in dist-git (that could have been already created by `propose_downstream` in a different branch for example).\\n\\nYou most likely want to enable the `sync_changelog` option so that your upstream `%changelog` is synchronized to dist-git as well.\\n\\n### Using a dummy value\\n\\nYou can use a value like _0_ or _1_ and never touch it, Packit will make sure it is set to _1_ in the dist-git spec file.\\nYou can do this if you don\'t care about upstream `%changelog`, i.e. you have `sync_changelog` disabled and use `copy_upstream_release_description` or the `changelog-entry` action to generate it downstream.\\n\\n### Using `%autorelease`\\n\\nYou can use the `%autorelease` macro in the `Release` field and the `%autochangelog` macro in `%changelog`. In this case Packit will not change anything, it will only synchronize changes made to the arguments of the macro (if any).\\n\\nIf you want to start using `%autorelease` and `%autochangelog`, you should [do the change](https://docs.pagure.org/fedora-infra.rpmautospec/opting-in.html) in dist-git before releasing a new version upstream and triggering `propose_downstream` job, otherwise Packit will continue to set `Release` to _1_ in the dist-git spec file.\\n\\n## `%autorelease` in dist-git\\n\\nNo matter how you maintain the `Release` field in your upstream spec file, Packit will never overwrite the `%autorelease` macro if it is used in the dist-git spec file."},{"id":"/pre-commit-hooks","metadata":{"permalink":"/posts/pre-commit-hooks","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/pre-commit-hooks/index.md","source":"@site/posts/pre-commit-hooks/index.md","title":"Packit\'s pre-commit hooks","description":"pre-commit is a wonderful tool that saves you","date":"2023-05-16T11:00:00.000Z","formattedDate":"May 16, 2023","tags":[{"label":"workflow","permalink":"/posts/tags/workflow"}],"readingTime":2.165,"hasTruncateMarker":true,"authors":[{"name":"Ji\u0159\xed Popelka","email":"jpopelka@redhat.com","url":"https://github.com/jpopelka","imageURL":"https://github.com/jpopelka.png","key":"jpopelka"}],"frontMatter":{"title":"Packit\'s pre-commit hooks","date":"2023-05-16T11:00:00.000Z","authors":"jpopelka","tags":["workflow"]},"prevItem":{"title":"Handling of Release field in propose_downstream job","permalink":"/posts/release-field-handling"},"nextItem":{"title":"2022 for Packit","permalink":"/posts/2022-features"}},"content":"[pre-commit](https://pre-commit.com) is a wonderful tool that saves you\\na lot of time by automatically checking your changes before you\\ncommit and/or push them out.\\n\\n\x3c!--truncate--\x3e\\n\\nFor example, in our [packit repo](https://github.com/packit/packit/blob/main/.pre-commit-config.yaml)\\nwe run various hooks upon each commit:\\n\\n- [Black (Python code formatter)](https://github.com/psf/black),\\n- [Prettier (code formatter)](https://github.com/prettier/prettier),\\n- [Flake8 (Python source code checker)](https://pypi.org/project/flake8),\\n- [Mypy (static type checker for Python)](https://github.com/python/mypy),\\n- and [several other pre-commit hooks](https://github.com/pre-commit/pre-commit-hooks).\\n\\n## Our pre-commit hooks\\n\\n[We also have a few hooks](https://github.com/packit/pre-commit-hooks)\\nwhich we\'ve created and which you might find useful as well.\\n\\n### check-rebase\\n\\nWe in Packit love linear git history.\\nThis hook checks whether your branch is up-to-date with the upstream,\\nand we use it to know when it\'s time to rebase changes before we push them.\\n\\nTo try, add this to your `.pre-commit-config.yaml`\\n\\n```yaml\\n- repo: https://github.com/packit/pre-commit-hooks\\n  rev: v1.2.0\\n  hooks:\\n    - id: check-rebase\\n      args: [upstream_url]\\n```\\n\\n### validate-config\\n\\nPackit uses a [YAML configuration file](https://packit.dev/docs/configuration)\\nin an upstream repository.\\nWe have a [packit validate-config](https://packit.dev/docs/cli/validate-config) command\\nto check it, but it\'s easy to forget (to run it) and notice a typo after you\\ncommitted and pushed the changes and waited for some time for Packit to tell you\\nthat in a PR.\\n\\nIt\'s much faster to catch the problem before committing and/or pushing the changes.\\n\\n#### validate-config-in-container\\n\\nThis hook runs (only if there\'s been a change in the `.packit.yaml`)\\n`packit` in a container (from [our image](https://quay.io/repository/packit/packit)),\\nmounts your sources inside and runs the `packit validate-config`.\\n\\n```yaml\\n- repo: https://github.com/packit/pre-commit-hooks\\n  rev: v1.2.0\\n  hooks:\\n    - id: validate-config-in-container\\n```\\n\\nIt requires docker/podman, which can be a problem for example in a\\n[CI](https://github.com/pre-commit-ci/issues/issues/11).\\n\\n#### validate-config\\n\\nAnother option is a hook which runs the `packit` binary directly installed\\non the machine. If there\'s no `packit`, the hook passes to not break\\nfor example your CI where `packit` is most likely not installed.\\n\\n```yaml\\n- repo: https://github.com/packit/pre-commit-hooks\\n  rev: v1.2.0\\n  hooks:\\n    - id: validate-config\\n```\\n\\n##### But why the hook doesn\'t install `packit` itself?\\n\\nRight, typically, when you run a pre-commit hook for the first time,\\nit installs everything it needs.\\nLike in case of Python, pre-commit `pip` installs all the dependencies.\\n\\nThe problem in our case is that `packit` has a lot of dependencies and\\nsome of them (if missing) are compiled from source when you try to\\n[pip install packit](https://packit.dev/docs/cli/#from-pypi).\\nThat needs `gcc` and additional `devel` packages\\nto be installed on the machine prior to running the hook for the first time.\\nThat would make the hook usage very user unfriendly, leaving aside that\\nsometimes (in a CI) you don\'t have access to the machine to install them\\nprior to running the hooks."},{"id":"/2022-features","metadata":{"permalink":"/posts/2022-features","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/2022-features/index.md","source":"@site/posts/2022-features/index.md","title":"2022 for Packit","description":"As you will see in the following paragraphs, the year 2022 was really fruitful for the Packit project. Without further ado, let\u2019s take a look at what the Packit team accomplished last year!","date":"2023-01-28T10:58:50.000Z","formattedDate":"January 28, 2023","tags":[{"label":"2022","permalink":"/posts/tags/2022"},{"label":"yearly-features","permalink":"/posts/tags/yearly-features"},{"label":"summary","permalink":"/posts/tags/summary"}],"readingTime":7.285,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Lachman","email":"flachman@redhat.com","url":"https://github.com/lachmanfrantisek","imageURL":"https://github.com/lachmanfrantisek.png","key":"flachman"}],"frontMatter":{"title":"2022 for Packit","date":"2023-01-28T10:58:50.000Z","authors":"flachman","tags":["2022","yearly-features","summary"]},"prevItem":{"title":"Packit\'s pre-commit hooks","permalink":"/posts/pre-commit-hooks"},"nextItem":{"title":"Automatic pulling of upstream releases to Fedora","permalink":"/posts/pull-from-upstream"}},"content":"As you will see in the following paragraphs, the year 2022 was really fruitful for the Packit project. Without further ado, let\u2019s take a look at what the Packit team accomplished last year!\\n\\n\x3c!--truncate--\x3e\\n\\n## Fedora automation\\n\\nWe have made a huge improvement in downstream automation. At the beginning of the year, we [finished the workflow](https://packit.dev/posts/downstream-automation/) and you are now able to use Packit to get your release from upstream via dist-git and Koji to Bodhi. As usual, you can pick just what you need. This workflow consists of three jobs:\\n\\n- `propose-downstream`: as a reaction to an upstream release, the source archive is saved to a lookaside cache, specfile is updated and sent as a pull request to Fedora dist-git.\\n- `koji-build`: as a reaction to a new dist-git commit, a new Koji build is triggered (you can specify allowed authors of a commit or merged pull request).\\n- `bodhi-update`: as a reaction to a successfully finished Koji build, a new Bodhi update is created\\n\\n![Detail of a Bodhi update created by Packit](img/bodhi-update.png)\\n\\nBut that wasn\u2019t all. At the very end of the year, the Packit team implemented an alternative to the `propose-downstream` job that we call `pull-from-upstream`. The logic of the job is the very same: the source archive is saved to a lookaside cache, specfile is updated and sent as a pull request to Fedora dist-git. The only \u2013 and main \u2013 difference is that the job is defined downstream (in the default dist-git branch, `rawhide` or its `main` alias) so you don\u2019t need to install Packit in the upstream repository. The information about a new release is received from the [Upstream Release Monitoring](https://docs.fedoraproject.org/en-US/package-maintainers/Upstream_Release_Monitoring). The `pull-from-upstream` job is mainly targeted to the Fedora maintainers without upstream access or with upstream not being supported by Packit. (This job works with any upstream using git.) The setup is nicely described in [this blog post](https://packit.dev/posts/pull-from-upstream). And if you want a dedicated documentation page for the Fedora downstream automation, look at https://packit.dev/docs/fedora-releases-guide/.\\n\\n![Pull request created by a pull-from-upstream workflow](img/pull-from-upstream.png)\\n\\nAnd that\u2019s still not all, we\u2019ve also added `propose-downstream` to [our dashboard](https://dashboard.packit.dev). You can now the jobs in [the Pipelines view](https://dashboard.packit.dev/pipelines). Also, logs can be checked on a detail page (that can be accessed from a status of a release commit).\\n\\n![Propose downstream result page](img/propose-downstream-result-page.png)\\n\\nStill not convinced we\u2019ve done a lot? When using various downstream jobs, you can still hit an issue now and then. (Messages from Fedora infrastructure can get lost or some intermittent error can occur.) Packit automatically retries to overcome temporary issues, but sometimes it\u2019s not enough or there is a real problem that needs to be fixed elsewhere. We can\u2019t resolve the real problems for you, but we can help let you retry the job when needed. And you have two places where to do that.\\n\\nAs you might be used to with the `propose-downstream` job, you can use comments in an upstream issue to retrigger Bodhi updates and Koji builds as well. Just configure [`issue_repository`](https://packit.dev/docs/configuration/#issue_repository) so Packit knows where to create issues in case of problems. (This does not need to be an upstream issue and this repository can be used for multiple projects.) Alternatively, if you use dist-git pull requests (either made by Packit or other maintainers), you can use a pull request comment to retrigger Koji Build or Bodhi update.\\n\\n![Comment to recreate a Bodhi update](img/dist-git-pull-request-comment.png)\\n\\nIf you are interested in what Packit has done for its users, you can take a look at the activity of the `packit` (or `packit-stg`) FAS user in [dist-git](https://src.fedoraproject.org/user/packit), [Koji](https://koji.fedoraproject.org/koji/userinfo?userID=4641) or [Bodhi](https://bodhi.fedoraproject.org/users/packit).\\n\\n![Dist-git activity of Packit user](img/packit-dist-git-activity.png)\\n![Koji builds triggered by Packit user](img/packit-koji-builds.png)\\n![Bodhi updates created by Packit user](img/packit-bodhi-updates-all.png)\\n\\n## SRPM in Copr\\n\\nThroughout the 2022 we have implemented support for building SRPMs in the Copr and slowly started introducing newly-onboarded projects to use Copr right from the start. The definitive switch to Copr has been done at the beginning of 2023. For more details, take a look at the [relevant blog post](https://packit.dev/posts/copr-srpms/), but let\u2019s shortly describe the benefits:\\nMost importantly, you can now use [`srpm_build_deps`](https://packit.dev/docs/configuration/#srpm_build_deps) config option to specify the dependencies needed for the build. With the [old solution](https://github.com/packit/sandcastle), this process was manual, global and had to be done by Packit maintainers. Another advantage of being able to use Copr for SRPM builds is access to the SRPM artifacts.\\n\\n## Self-mapping of FAS account\\n\\nFrom the beginning of the Packit project, for every Packit GitHub installation, we have required a Fedora account so we can be sure, that we are safe to use Fedora systems (like Copr or dist-git) on behalf of that user. Since we automate various tasks for you, we\u2019ve made it possible for you to perform this yourself. (Thanks to the user config field in the Fedora Account System.)\\nIf you are interested in how this works, take a look at the blog post we\u2019ve prepared for you: https://packit.dev/posts/fas-verification-automation/\\n\\nIf you wonder why we check permissions for the installation, we have good news for you. This year, we plan to improve the permission schema, do the checks for each job, and require only what is needed. But more about the plans for 2023 later.\\n\\n![Self-mapping process in an issue](img/verify-fas.png)\\n\\n## Multiple-project test runs\\n\\nIf you have multiple connected projects (as we do) and work on a feature spanning more of them, the following feature might come in handy. You can use a commit command with a reference to the other pull request and Packit will use Copr builds from both pull requests during the tests. Want to know more? Check this blog post: https://packit.dev/posts/testing-farm-triggering/\\n\\n![Test with external build](img/test-for-external-build.png)\\n\\n## Specfile library\\n\\nThis one might not be relevant to most of our users, but we would still like to announce, that we\u2019ve created [a Python library](https://github.com/packit/specfile) for specfile manipulation. It can not only parse various weird spec files but also can edit them with as little diff as possible. Also, the code is really interesting so check it out. Now, it\u2019s used by Packit and [rebase-helper](https://github.com/rebase-helper/rebase-helper) and you can watch [this demo](https://www.youtube.com/watch?v=yzMfBPdFXZY&t=17s) if you want to know more.\\nIn case you\u2019ve missed that, this is not the first time we\u2019ve extracted a part of our codebase for wider usage \u2013 another nice example is a [forge-independent Python library for GitHub/GitLab/Pagure API called OGR](https://github.com/packit/ogr/).\\n\\n## VM Image Builds\\n\\nHaving Copr builds available for your pull-requests is really nice, but you can now have VM image builds as well. It would be really wasteful to do this for each and every commit so we decided to trigger this by a `/packit vm-image-build ` comment. Similarly to other jobs, Packit uses an external system to do the hard work. This feature is possible thanks to the [Red Hat Image Builder](https://console.redhat.com/insights/image-builder). This feature is a fresh start on this journey and we are investigating the possibilities of Packit in this field. Check [our documentation](https://packit.dev/docs/configuration/upstream/vm_image_build) and let us know what you think.\\n\\n![VM image build as a result of the comment](img/vm-image-build.png)\\n\\n## Plans for the next year\\n\\nDo you wonder what we plan for you for the year 2023? That is not a secret. For some time, we have been opening our planning and since the last year, you can check our [Kanban board](https://github.com/orgs/packit/projects/7/) since it has become the primary place we use on daily bases. (It used to be half-manually and half-automatically synced.)\\nEvery quarter, our team sits down and discusses our plans for the next three months. We use MOSCOW (=MUST x SHOULD x COULD x WON\u2019T) prioritisation method to group and compare all of our epics. The result of this time\u2019s planning can be seen [here](https://github.com/orgs/packit/projects/7/views/25).\\n\\nDo you want to know how we decide and how you can influence us? It\u2019s pretty easy. Firstly, we need to know about a bug or feature request. Secondly, the task needs to be worth the work. (And should be related to our mission.) We can\u2019t work on everything so we need to pick the work with a bigger impact (a lot of projects will benefit from that or a significant benefit for a smaller group). So shortly, if you want something to be done: create an issue, provide a clear reasoning why we should do that and find other projects that can benefit from that.\\n\\nAnd also, our project is open source. We are more than happy to help anyone contribute to our code base!\\n\\n---\\n\\nWith that, I, personally, would like to thank all of the Packit team members for their outstanding work during the year. And I also would like to thank you, our users, for using our project, being kind, helpful and patient! I wish you all a happy new year and less mundane work as possible!\\n\\nOn behalf of the Packit team,\\nFranti\u0161ek"},{"id":"/pull-from-upstream","metadata":{"permalink":"/posts/pull-from-upstream","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/pull-from-upstream/index.md","source":"@site/posts/pull-from-upstream/index.md","title":"Automatic pulling of upstream releases to Fedora","description":"In the previous year, we automated the Fedora downstream release process in Packit.","date":"2023-01-23T08:23:44.000Z","formattedDate":"January 23, 2023","tags":[{"label":"downstream","permalink":"/posts/tags/downstream"}],"readingTime":3.94,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"Automatic pulling of upstream releases to Fedora","date":"2023-01-23T08:23:44.000Z","authors":"lbarczio","tags":["downstream"]},"prevItem":{"title":"2022 for Packit","permalink":"/posts/2022-features"},"nextItem":{"title":"Running tests with builds from another PR","permalink":"/posts/testing-farm-triggering"}},"content":"In the previous year, we [automated](/posts/downstream-automation) the Fedora downstream release process in Packit.\\nThe first step of the release process, propagating the upstream release to Fedora,\\nis covered by the [`propose_downstream`](/docs/configuration/upstream/propose_downstream) job.\\nThis job updates the sources in Fedora, the spec file, and other needed files and creates pull requests with the changes\\nin the dist-git repository.\\n\\nThe downside of this job is that for its execution, users need to install Packit Service GitHub/GitLab\\napp since this job reacts only to GitHub/GitLab release webhooks.\\nHowever, the person who maintains the package in Fedora may not be the upstream maintainer and may not have admin access\\nto the upstream GitHub/GitLab repository.\\n\\nTo cover this case, we came up with a new job called `pull_from_upstream`, which aims to update Fedora dist-git similarly\\nto `propose_downstream`, but is configured directly in the dist-git repository.\\nLet\'s now look at how to set it up and how it works.\\n\\n\x3c!--truncate--\x3e\\n\\n## Setup\\n\\n### Upstream release monitoring\\n\\n`pull_from_upstream` job reacts to a new bug in [Bugzilla](https://bugzilla.redhat.com/) about a new upstream version\\nof a project. The bug is automatically created by\\n[Upstream Release Monitoring](https://docs.fedoraproject.org/en-US/package-maintainers/Upstream_Release_Monitoring/).\\nTo enable the Upstream Release Monitoring:\\n\\n1. Add the upstream project (if it is not there yet)\\n   to [Anitya](https://release-monitoring.org/) and configure the mapping to a Fedora package:\\n   ![Project in Anitya](img/anitya-project.png)\\n\\n2. Enable the monitoring in the dist-git repository ([Fedora Package Sources](https://src.fedoraproject.org)):\\n\\n![Monitoring in dist-git](img/dist-git-monitoring.png)\\n\\n:::info\\n\\nIn Anitya, there are multiple backends you can configure the mapping for.\\nBesides GitHub or GitLab, you can use e.g. PyPI, pagure, or\\n[many others](https://release-monitoring.org/static/docs/user-guide.html#backends).\\nAlso, be aware that there can be a delay in retrieving the new version,\\nso the update to Fedora is usually not created instantly (e.g. for Python projects,\\nit is better to configure PyPI backend rather than GitHub since the monitoring\\nthere is much less delayed).\\n\\n:::\\n\\n### Packit configuration\\n\\nTo automatically pull the upstream release as a reaction to the bug in Bugzilla, `pull_from_upstream` job\\ntogether with the `upstream_project_url` configuration option, needs to be defined in the default branch\\n(`rawhide`) of the dist-git repository in the Packit configuration file (see\\n[our documentation](/docs/configuration/downstream/pull_from_upstream)). The [`upstream_project_url`](/docs/configuration/#upstream_project_url) needs to be a URL\\npointing to a Git repository so that we can do `git` commands on it.\\n\\n## `pull_from_upstream` in action\\n\\nLet\'s showcase the new job in action for the latest release of\\n[Packit itself](https://pypi.org/project/packitos/).\\n\\nAs you can see in the `Setup` section above, the Upstream Release Monitoring is configured:\\nthere is a PyPI project `packitos` in Anitya\\nwith configured mapping to the Fedora package `packit` and the monitoring in the\\n[`packit` dist-git repository](https://src.fedoraproject.org/rpms/packit) is enabled.\\nWe could configure the mapping in Anitya from the GitHub project directly instead, and it would work as well. Just be aware that\\nfor each Fedora package, there can be a mapping only from one project.\\n\\nIn Packit configuration file, we have configured the job and related options:\\n\\n```yaml\\nupstream_project_url: https://github.com/packit/packit\\nissue_repository: https://github.com/packit/packit\\ncopy_upstream_release_description: true\\n\\njobs:\\n  - job: pull_from_upstream\\n    trigger: release\\n    dist_git_branches:\\n      - fedora-all\\n      - epel-8\\n```\\n\\nYou can see that version `0.66.0` of Packit (`packitos` in PyPI) was released:\\n\\n![Packit release PyPI](img/packit-release-pypi.png)\\n\\nWhen Upstream Release Monitoring retrieved this new version, it created a new bug:\\n\\n![Bugzilla](img/packit-release-bugzilla.png)\\n\\nThis triggered Packit, and after checking the Packit configuration in dist-git\\nand finding the `pull_from_upstream` job, this job was run.\\n\\nUsing the `upstream_project_url` from the configuration, Packit was able to get the needed information\\nfrom the corresponding GitHub release:\\n![Packit release GitHub](img/packit-gh-release.png)\\n\\nAs a result, pull requests for configured branches were created.\\nHere is an example of one of the created pull requests and part of its content:\\n![Dist-git PR](img/pull-from-upstream-pr.png)\\n![Pull request content](img/pull-from-upstream-content.png)\\n\\nSince we have configured the [`issue_repository`](/docs/configuration#issue_repository), we could be\\nalso notified about errors:\\n![Pull from upstream issue](img/pull-from-upstream-issue.png)\\n\\n:::tip Update July 2023\\n\\nIt is now also possible to retrigger the job, see\\n[the details](/docs/configuration/downstream/pull_from_upstream#retriggering).\\n\\n:::\\n\\nAlso, if you need to do any change in the pull request, you need to locally fetch the source branch\\nof the Packit\'s pull request and push it (with a fix) to your fork (as it is not possible to push to the branch\\ncreated in the Packit\'s fork):\\n\\n    git fetch ssh://$USER.fedoraproject.org/forks/packit/rpms/$YOUR_PACKAGE.git refs/heads/*:refs/remotes/packit/*\\n    git cherry-pick packit/$VERSION-$BRANCH-update-pull_from_upstream\\n\\n## Few words in the end\\n\\n`pull_from_upstream` has just been implemented; therefore, we encourage you to help\\ntest it out and make it perfect! There are still some limitations (e.g. regarding upstreams,\\nsee [documentation](/docs/configuration/downstream/pull_from_upstream)), which we are trying to resolve as soon as possible.\\nWe believe this functionality\\ncould be beneficial for maintainers of Fedora packages and could even be integrated further.\\nAny [suggestions](https://github.com/packit/packit-service/issues/new) and feedback are welcomed\\n(see [contacts](/#contact)).\\n\\nIf you are interested in details of customization of\\nthe `pull_from_upstream` job and in the whole downstream automation, make sure to check out\\n[our Fedora release guide](/docs/fedora-releases-guide) as well!"},{"id":"/testing-farm-triggering","metadata":{"permalink":"/posts/testing-farm-triggering","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/testing-farm-triggering/index.md","source":"@site/posts/testing-farm-triggering/index.md","title":"Running tests with builds from another PR","description":"Do you contribute to projects which depend on each other?","date":"2022-12-21T00:00:00.000Z","formattedDate":"December 21, 2022","tags":[{"label":"testing-farm","permalink":"/posts/tags/testing-farm"}],"readingTime":2.185,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Ne\u010das","url":"https://github.com/FrNecas","imageURL":"https://github.com/FrNecas.png","key":"fnecas"}],"frontMatter":{"title":"Running tests with builds from another PR","date":"2022-12-21T00:00:00.000Z","authors":"fnecas","tags":["testing-farm"]},"prevItem":{"title":"Automatic pulling of upstream releases to Fedora","permalink":"/posts/pull-from-upstream"},"nextItem":{"title":"Automation of FAS verification in Packit Service","permalink":"/posts/fas-verification-automation"}},"content":"Do you contribute to projects which depend on each other?\\nWould you like to test changes spanning multiple repositories together before merging them to the main branch?\\nThen look no further, Packit\'s new feature of the Testing Farm integration is what you are looking for!\\n\\n\x3c!--truncate--\x3e\\n\\n### How it works\\n\\nTo enable such testing, there is no additional configuration required in your `packit.yaml`, the typical [Testing Farm configuration](/docs/testing-farm/) is sufficient.\\nOnce you open a pull request with some changes, tests are going to run as usual with all dependencies being installed based on the test definition, e.g. from Fedora repositories.\\nTo trigger tests with builds from a pull request in another repository, add a comment to the pull request of the form:\\n\\n    /packit test <namespace>/<repo>#<pr-id>\\n\\nBased on this comment, Testing Farm will first install the recent successful builds created by Packit in the given pull request and then run the tests.\\nIn order for this to work, there must be successful builds for the targets that you are running tests for.\\nFor example, if you are testing against Fedora 36, the pull request that you want to install builds from must contain a successful Fedora 36 build by Packit.\\n\\nLet\'s look at a simple example to demonstrate this feature better.\\nThe Packit CLI uses a library called `specfile` to modify RPM spec files.\\nRecently, `specfile` has added a new feature which makes accessing the `Epoch` field in the spec file more convenient and we would like to make use of this feature.\\nHowever, the changes have not made it to a Fedora release yet, trying to use this feature will result in an error:\\n\\n![Tests in Testing Farm fail](img/testing_farm_failed.png)\\n\\n![Test log in Testing Farm](img/testing_farm_failed_log.png)\\n\\nAs we can see in the screenshots, during artifact installation, the latest `specfile` release from Fedora was installed, however it lacks the feature that we are looking to test.\\nLet\'s now retrigger the tests, but specify that we want to install builds from the pull request in `specfile` which introduced the changes:\\n\\n![Retriggering tests with builds from another PR](img/testing_farm_retrigger.png)\\n\\nHooray! The copr builds from PR 165 were installed before the tests were run in Testing Farm which enabled us to test the feature inside Packit CLI.\\n\\n### Wrapping up\\n\\nWe hope that this new feature makes upstream testing even more convenient than it previously was.\\nThe feature is still quite new, and we would love to hear what you think about it.\\nAs always, if you run into any trouble or have any ideas how to improve this functionality,\\ndo not hesitate to [reach out to us](/#contact).\\nWe will be happy to help."},{"id":"/fas-verification-automation","metadata":{"permalink":"/posts/fas-verification-automation","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/fas-verification-automation/index.md","source":"@site/posts/fas-verification-automation/index.md","title":"Automation of FAS verification in Packit Service","description":"As you may already know, for using Packit Service","date":"2022-05-24T00:00:00.000Z","formattedDate":"May 24, 2022","tags":[{"label":"fas","permalink":"/posts/tags/fas"}],"readingTime":2.055,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"Automation of FAS verification in Packit Service","date":"2022-05-24T00:00:00.000Z","authors":"lbarczio","tags":["fas"]},"prevItem":{"title":"Running tests with builds from another PR","permalink":"/posts/testing-farm-triggering"},"nextItem":{"title":"Downstream automation is here","permalink":"/posts/downstream-automation"}},"content":"As you may already know, for using Packit Service\\nGitHub App we [require our users to have a valid Fedora Account System account](/docs/guide/#2-approval).\\nWe were verifying the newcomers until now manually, but in recent weeks, we have implemented an automated solution\\nfor it. Let\'s take a closer look at how it is done currently and what have we improved!\\n\\n\x3c!--truncate--\x3e\\n\\nFormerly, the process of verification by us started by waiting for the users to provide\\nus their FAS username, then checking whether the provided FAS account\\nexists and matches, and finally, manually adding the account to our allowlist in the database. For the communication with\\nnew users, we have used our [`packit/notifications`](https://github.com/packit/notifications/issues)\\nrepository on GitHub where we created an issue for each new installation.\\n\\nAlthough in general, this worked, it required human interaction and since we are not available 24/7, the verification wasn\'t immediate.\\nWe wanted to simplify the process for both users and us.\\nSince in FAS, everyone can set their GitHub login that is then publicly available, we decided to utilize this setting.\\n\\nSo how does the verification work now?\\n\\nFor each new GitHub installation, we first check whether there isn\'t a FAS account with the same login as the one\\nthat triggered the installation. If we find such an account, we check whether the `GitHub Username` in this FAS account matches\\nthe GitHub login of the one that triggered the installation. To get the information about the FAS accounts, we\\nuse the [`fasjson-client` library](https://github.com/fedora-infra/fasjson-client).\\nIf this check doesn\'t prove any match, we create an issue in the\\n`packit/notifications` repository as previously. This is what it looks like:\\n\\n![Issue in packit/notifications](img/notifications-repo-issue.png)\\n\\nAs you can see, it contains instructions on how to trigger the verification automatically. So, everything the person who installed the app needs to do is set the `GitHub Username` field in their FAS account (if they don\'t have it set\\nalready) and then provide the FAS login via Packit comment command `/packit verify-fas the-fas-account`.\\n\\nOnce the user does this, our service runs the same verification again (with the FAS username provided in the command) and informs users about the status\\nvia a comment in the same issue. The successful verification looks like this:\\n\\n![Successful verification](img/verify-fas.png)\\n\\nThis should save both users and our time and hopefully make the onboarding process smoother for the newcomers.\\nSince this is a pretty new feature,\\nlet us know whether there is something that is not clear so that we can improve it."},{"id":"/downstream-automation","metadata":{"permalink":"/posts/downstream-automation","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/downstream-automation/index.md","source":"@site/posts/downstream-automation/index.md","title":"Downstream automation is here","description":"Finally, it\'s here. Now, you can do the whole Fedora release with the help of Packit.","date":"2022-05-06T00:00:00.000Z","formattedDate":"May 6, 2022","tags":[{"label":"propose-downstream","permalink":"/posts/tags/propose-downstream"}],"readingTime":3.265,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Lachman","email":"flachman@redhat.com","url":"https://github.com/lachmanfrantisek","imageURL":"https://github.com/lachmanfrantisek.png","key":"flachman"}],"frontMatter":{"title":"Downstream automation is here","date":"2022-05-06T00:00:00.000Z","authors":"flachman","tags":["propose-downstream"]},"prevItem":{"title":"Automation of FAS verification in Packit Service","permalink":"/posts/fas-verification-automation"},"nextItem":{"title":"Building SRPMs in Copr","permalink":"/posts/copr-srpms"}},"content":"Finally, it\'s here. Now, you can do the whole Fedora release with the help of Packit.\\nLet\'s take a look at how it works on an example of [OGR](https://github.com/packit/ogr), the Python library we develop.\\n\\n\x3c!--truncate--\x3e\\n\\n## Upstream\\n\\nThe process of releasing a new version starts in the upstream repository.\\nHere, we can see an upstream release:\\n\\n![Upstream release](img/upstream_release.png)\\n\\n## Propose downstream\\n\\nAs the first step on our way to Fedora users, we need to get the new upstream release to the Fedora dist-git.\\nThis is what we call `propose-downstream` job.\\nHere is a snippet from the config file of OGR:\\n\\n```yaml\\ndownstream_package_name: python-ogr\\ncopy_upstream_release_description: true\\n\\njobs:\\n  - job: propose_downstream\\n    trigger: release\\n    dist_git_branches:\\n      - fedora-all\\n      - epel-8\\n```\\n\\nHow does the `propose-downstream` work?\\nAs a first step, the archive is saved to lookaside cache\\nand after that, Packit updates the dist-git content (mainly `sources` file and spec-file)\\nvia pull-requests for the specified branches. (Direct push is possible only for CLI by setting a\\n[`create_pr` option](https://packit.dev/docs/configuration/#create_pr) to `false`.)\\n\\nIf you use [`copy_upstream_release_description: true`](https://packit.dev/docs/configuration/#copy_upstream_release_description),\\nas in the config above,\\nthe changelog entry will use the GitHub/GitLab release description field.\\n(Just make sure the formatting is compatible with spec-file.\\nE.g. use `-` instead of `*` for lists to not create multiple changelog entries.)\\n\\nAnd how is it triggered?\\nPackit gets the information about the newly created release from GitHub/GitLab (via webhook),\\nloads the config from the release commit and if there is a `propose-downstream` job\\ndefined, the workflow begins.\\n\\nHere are the pull-requests created by Packit:\\n\\n![List of downstream pull-requests created by Packit](img/distgit_prs.png)\\n\\nAnd here are the details of the one created for `f35` branch:\\n\\n![Downstream pull-request created by Packit](img/distgit_pr_detail.png)\\n![Downstream pull-request created by Packit: changes in specfile](img/distgit_pr_specfile.png)\\n\\nNow, it\'s on downstream CI systems and maintainer to check the changes and merge\\nthe pull-request.\\n\\n## Koji\\n\\nIf Packit sees a new commit in the configured dist-git branch, it submits a new build in Koji\\nlike maintainers usually do. (The commits without any spec-file change are skipped.)\\n\\n![List of Koji builds triggered by Packit](img/koji_builds.png)\\n\\nHere is a job definition for the package we use as an example:\\n\\n```yaml\\njobs:\\n  - job: koji_build\\n    trigger: commit\\n    dist_git_branches:\\n      - fedora-all\\n      - epel-8\\n```\\n\\nThere is no UI provided by Packit for the job,\\nbut it is visible across Fedora systems (like you can see in the following image)\\nlike a manually created Koji build and you can utilise\\n[Fedora Notifications](https://apps.fedoraproject.org/notifications/about)\\nto get informed about the builds.\\n\\n![Build status in merged downstream pull-request](img/distgit_pr_build_status.png)\\n\\n## Bodhi\\n\\nOnce Packit is informed (via fedora-messaging bus) about the successful Koji build,\\nit creates a new update in Bodhi for you.\\n\\n![List of Bodhi updates created by Packit](img/bodhi_updates.png)\\n\\nHere is a job definition:\\n\\n```yaml\\njobs:\\n  - job: bodhi_update\\n    trigger: commit\\n    dist_git_branches:\\n      - fedora-branched # rawhide updates are created automatically\\n      - epel-8\\n```\\n\\nThe packit config is loaded from the commit the build is triggered from.\\n\\nHere is an example of the resulting Bodhi update:\\n\\n![Bodhi update created by Packit](img/bodhi_update_detail.png)\\n\\nAnd that\'s all. The rest is on the users and maintainers to give the update enough Karma\\nso the update gets to the users.\\n\\n## Conclusion\\n\\nDoes it look simple? Yes, it is. We try to automate\\nas much as possible but still leave the space for human intervention where it is needed --\\npull-request review and verification of the Bodhi update.\\nOf course, in case of some errors, a human can (and should)\\nreplace the work of a bot.\\nOther manual, mundane and waiting tasks are replaced by Packit.\\n\\nPlease, try it yourself and let us know what do you think.\\nThose jobs are really new and some issues might occur.\\nBut we will try to fix those and if you have any suggestions\\nfor improvement, please, [create an issue](https://github.com/packit/packit-service/issues/new/choose) so we can\\nsee if the request is doable and we can try to implement it.\\nAnd of course, code contribution is more than welcome as well."},{"id":"/copr-srpms","metadata":{"permalink":"/posts/copr-srpms","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/copr-srpms/index.md","source":"@site/posts/copr-srpms/index.md","title":"Building SRPMs in Copr","description":"Let\'s find out how Packit builds your SRPMs in the Copr.","date":"2022-03-07T11:57:40.000Z","formattedDate":"March 7, 2022","tags":[{"label":"srpm","permalink":"/posts/tags/srpm"},{"label":"copr","permalink":"/posts/tags/copr"}],"readingTime":4.795,"hasTruncateMarker":true,"authors":[{"name":"Laura Barcziov\xe1","email":"lbarczio@redhat.com","url":"https://github.com/lbarcziova","imageURL":"https://github.com/lbarcziova.png","key":"lbarczio"}],"frontMatter":{"title":"Building SRPMs in Copr","date":"2022-03-07T11:57:40.000Z","authors":"lbarczio","tags":["srpm","copr"]},"prevItem":{"title":"Downstream automation is here","permalink":"/posts/downstream-automation"},"nextItem":{"title":"2021 for Packit","permalink":"/posts/2021-features"}},"content":"Let\'s find out how Packit builds your SRPMs in the Copr.\\n\\n\x3c!--truncate--\x3e\\n\\n### Introduction\\n\\nIf you use Packit to build RPMs for your upstream code changes, likely,\\nyou have already read about how does Packit build your [SRPMs](http://ftp.rpm.org/max-rpm/s1-rpm-miscellania-srpms.html).\\nIf not, then just a short recap:\\nEach time an RPM build is triggered, Packit builds an SRPM and then submits\\nthe created SRPM file to Copr where Copr takes care of building the actual RPMs.\\nSince you can modify the behaviour of building SRPMs by defining [actions](/docs/configuration/actions/),\\nthis process needs to be run in an isolated environment. For this, we implemented our\\n[sandboxing mechanism](https://github.com/packit/sandcastle),\\nwhich simply runs the provided commands in an Openshift pod freshly created for each build.\\n\\n#### Problems of the previous workflow for SRPM builds\\n\\nThis is a pretty good-functioning workflow, but it has some downsides which have become more and more annoying\\nwith the growing user base. Because of the resources, we have set limits for Openshift pods running at one time.\\nThis directly affects how many SRPM builds can run in parallel. As a result, when there are too many requests for (S)RPM builds,\\nsome can get stuck in the queue while waiting for other builds to finish. Another inconvenience coming with pods being\\nalways freshly created is copying the needed data into\\nand from the pod. This has also cost us some months of desperate debugging of weird errors.\\nAnother disadvantage is that users cannot easily configure dependencies for their actions run during building SRPMs.\\nWe have to install the dependencies manually on-demand, but of course, which is not flexible.\\n\\nWe were thinking about improving the process for a long time but never reached any clear conclusion.\\nThen in one of our architecture meetings, when we tried to solve another\\nissue related to our sandboxing solution, Pavel Raiskup from Copr team asked us why didn\'t\\nwe build the SRPMs directly in Copr. We knew that there is a way of building SRPMs in Copr,\\nbut weren\'t aware of the details and how would this fit our use case.\\n\\n### Implementation of the Copr SRPMs\\n\\nAfter some research of the [Copr custom source method](https://docs.pagure.org/copr.copr/custom_source_method.html), we\\ndecided to give it a try. To make Copr build the SRPMs, Copr needs to be provided\\nwith a script that will prepare the sources used to build an SRPM.\\nTherefore, we created the `packit prepare-sources` command, which mostly reuses existing code that is run also in the sandbox\\nworkflow. It prepares the specfile, archive and other sources and then moves\\nthem to a separate directory. So with the new implementation,\\nwith each request to run (S)RPM build, Packit sends a dynamically created \\"script\\" to Copr that invokes our new command.\\nHere is what the script can look like:\\n\\n    #!/bin/sh\\n    git config --global user.email \\"hello@packit.dev\\"\\n    git config --global user.name \\"Packit\\"\\n    resultdir=$PWD\\n    packit -d prepare-sources --result-dir \\"$resultdir\\" --pr-id 676 --job-config-index 2 https://github.com/packit/ogr\\n\\nYou can see that a pull request should be checked out or which job defined\\nin your Packit job config is the trigger of this action. And that\'s it! Copr finds the sources and builds SRPM from them.\\nPackit listens to the messages about the start and end of the build and similarily as for RPM builds, reports the\\nstate via commit statuses/checks and provides the URL with the logs.\\n\\n### Deployment phases\\n\\nSince this change is pretty significant, we wanted to start using this workflow gradually and catch all the problems\\nbefore we get rid of the previous workflow for SRPMs.\\nAt first, we tested how does the new solution work in our projects. The only disadvantage\\nwas that the actual build process\\ntakes a little longer than in sandcastle as we get an isolated environment where all the packages are installed for each new build.\\nOn the other hand, Copr usually starts the build very soon after it is submitted, so no long wait time until some other build is finished.\\nIn the initial implementation, we installed a list of dependencies\\n[which are present](https://github.com/packit/sandcastle/blob/ece539650770fea057877f0c97074acf506fada4/files/install-rpm-packages.yaml#L5) in our\\nsandbox which also increased the build time a bit.\\n\\nSo as the following step we added the functionality to define dependencies for actions in the\\nPackit config file with [`srpm_build_deps` key](/docs/configuration/#srpm_build_deps).\\n\\nExample of how the configuration of `srpm_build_deps` can look like:\\n\\n```yaml\\nactions:\\n  create-archive:\\n    - \\"python3 setup.py sdist --dist-dir .\\"\\n    - \\"sh -c \'echo packitos-$(python3 setup.py --version).tar.gz\'\\"\\n  get-current-version:\\n    - \\"python3 setup.py --version\\"\\n\\nsrpm_build_deps:\\n  - python3-pip\\n  - python3-setuptools_scm\\n```\\n\\nWe also decided that presence of this key in the config will be for some period an indicator to build the SRPMs in Copr. With this approach, anyone can configure\\ntheir dependencies and play with adding and adjusting them as needed without directly breaking\\nthe builds in their repository. When the builds in the PR pass, the configuration change can be merged and the new approach\\nwill be used for the whole repository. We wanted\\nto kick off this process and therefore started opening PRs with dependencies configuration for projects that use\\nthe RPM builds functionality the most. During this phase, you can reach out to us with your feedback, so we can\\nimprove it even more!\\n\\nAs a next step, we use the new approach for GitHub app installations made since September 6, 2022.\\n\\nAnd as of January 10th 2023, we switched to building all SRPMs in Copr\\nand thus got rid of using our sandbox for building SRPMs entirely.\\n\\nSince we don\'t want to break your CI results because of missing dependencies, we will use the previously linked list of deps.\\nAs the list is pretty long, we encourage you to define your dependencies on your own. If you\\nbump into any troubles with setting up SRPM builds in Copr, please,\\n[reach out to us](/#contact), we will be glad to help!"},{"id":"/2021-features","metadata":{"permalink":"/posts/2021-features","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/2021-features/index.md","source":"@site/posts/2021-features/index.md","title":"2021 for Packit","description":"The previous year 2021 wasn\'t interesting only because of the increased usage of Packit","date":"2022-01-12T08:23:38.000Z","formattedDate":"January 12, 2022","tags":[{"label":"2021","permalink":"/posts/tags/2021"},{"label":"yearly-features","permalink":"/posts/tags/yearly-features"},{"label":"summary","permalink":"/posts/tags/summary"}],"readingTime":5.395,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Lachman","email":"flachman@redhat.com","url":"https://github.com/lachmanfrantisek","imageURL":"https://github.com/lachmanfrantisek.png","key":"flachman"}],"frontMatter":{"title":"2021 for Packit","date":"2022-01-12T08:23:38.000Z","authors":"flachman","tags":["2021","yearly-features","summary"]},"prevItem":{"title":"Building SRPMs in Copr","permalink":"/posts/copr-srpms"},"nextItem":{"title":"2021 in Numbers","permalink":"/posts/2021-in-numbers"}},"content":"The previous year 2021 wasn\'t interesting only because of the increased usage of Packit\\n(you can see more in [the previous post](/posts/2021-in-numbers)).\\nThe whole Packit team made a lot of improvements during the year.\\nSome small, some really big. So, let\'s take a look at the most important ones!\\n\\n\x3c!--truncate--\x3e\\n\\n## Dashboard\\n\\nThe idea of having a dashboard for Packit service started as a\\n[Google Summer of Code 2020 project](https://communityblog.fedoraproject.org/gsoc-progress-report-dashboard-for-packit-july-1-aug-16-2020/)\\nto provide a basic view of our service.\\nThanks [Anchit](https://github.com/IceWreck) for starting this!\\nNowadays, it\'s a core part of the project and it has replaced the result pages in plain HTML. Do you remember them?\\n\\nThe dashboard can be found at [dashboard.packit.dev](https://dashboard.packit.dev).\\n\\nFollowing picture shows a more convenient and visually-appealing view of builds and test runs.\\nFor better context, the relevant pages are a connected to each other.\\n\\n![Dashboard: Copr Build result](img/dashboard-copr.png)\\n\\nIf you want to see the overall picture, use our [pipelines view](https://dashboard.packit.dev/pipelines) that was created exactly for that:\\n\\n![Dashboard: Pipelines view](img/dashboard-pipelines.png)\\n\\n### Future of the dashboard\\n\\nWe consider our dashboard an important part of our service and are working on or planning more improvements:\\n\\n- We are working on personalised pages for a user or git-forge namespace.\\n- We are planning to show info about other job types we support by the service as well;\\n  especially the `propose-downstream` one.\\n- We are doing some database schema updates to be able to better interconnect various pages.\\n- Do you have an idea for an improvement?\\n  Let us know by creating an issue [here](https://github.com/packit/dashboard/issues/new).\\n\\n## Development\\n\\nFrom the very start, Packit is developed publicly in an open-source way.\\nWe participate in various projects like Google Summer of Code, Red Hat Open Source Contest and Hacktoberfest.\\nBut we are also very glad if anyone from our users contributes and fixes some pain point.\\nTo help with that, we\'ve renamed all our branches to `main` and rapidly enhanced our contribution guide(s).\\nWe would like to encourage you not to be afraid of contributing to any of [our projects](https://github.com/packit/).\\nWe are prepared to help you with that.\\n\\nIf you want to keep an eye on what we are currently working on, check our [_Packit upstream work_ board on GitHub](https://github.com/orgs/packit/projects/4).\\n\\n![GitHub: Packit upstream work](img/github-project.png)\\n\\n## Testing Farm\\n\\nThe year 2021 was a tough one for our test workflow. For those who don\'t know, we use Testing Farm as our test runner.\\nAt the beginning of the year, we switched to the new Testing Farm API version\\n(because the old one had died with the infrastructure it had been running on).\\nUnlike the old version, the new one fully supports `tmt` as a test definition.\\n\\n![Testing Farm: results](img/testing-farm-results.png)\\n\\nDuring the year, a set of supported environments was enhanced by `centos-6`, `oraclelinux` and `aarch64`.\\nFor Red Hat teams, we added support for using the internal instance of the Testing Farm.\\nLet us know if you are interested in this.\\nBut no worries, you can use `centos-stream` and other publicly available environments.\\n\\nOriginally, the tests were run after the installation of the packages built using Copr from the source repository.\\nNewly, you can skip this step and run the tests without any build. This allows you to use Packit&TestingFarm\\nfor repositories containing only test definitions (e.g. QE teams).\\n\\n![Testing Farm: no build](img/tf-no-build.png)\\n\\nLastly, we send some environment variables to the test environment and you can define your own if you want.\\n\\nAs we see, testing is a key feature for some teams and we still want to improve test use-cases\\n-- let us know if you are missing anything ([here](https://github.com/packit/packit-service/issues/new) or\\n[in the Testing Farm issue tracker](https://gitlab.com/groups/testing-farm/-/issues)).\\n\\n## Service\\n\\nTo get users quickly know that we accepted the task and started working on it, we added two nice features\\n-- `:1` reaction for the comment that we are reacting on and `task accepted` commit status.\\n\\n![Packit Service: reactions](img/thumbs-up.png)\\n\\nSpeaking of statuses, we switched to a more feature-rich API called GitHub Check Runs.\\nIt allows us to create a separate result page where we can show more information\\n-- e.g. more links when needed and more space for hints when there is a problem.\\nYou can also find the run results on a separate `Checks` tab of the pull-requests page.\\nThe check run page contains only the basic info and we don\'t want to replace a dashboard with this.\\n(Because of the consistency between git-forges and to be able to link the related dashboard pages.)\\nAnother feature of check runs you might find useful is being able to re-run the failed test with just one click\\n(see the `Re-run` button in the following screenshot).\\n\\n![Check runs](img/check-runs.png)\\n\\nInspired by other systems (like Zuul), for pull-requests we started using merge state\\nso you can be sure the state we use is the same as the one with the pull-request being merged.\\nWe are working with the Testing Farm team to add the support there as well so the test definition is consistent with the build.\\nBut no worries, you can disable this if you don\'t want this behaviour.\\n\\n## Downstream\\n\\nOne of our current initiatives is to help maintainers in the downstream part of the workflow as well.\\nWe had the first part of that for some time in a form of `propose-downstream` job (you can expect more enhancements on this front),\\nbut we newly support triggering Koji builds for new commits in dist-git.\\n\\nWhen there is a new dist-git commit that contains Packit config with the defined `koji_build` job,\\nPackit will trigger the Koji build for you.\\nIt\'s fresh and basic so far so give us some time to announce this with more details.\\nThe next step will be to create a Bodhi update when the build successfully finishes and that is\\nwhat is currently being worked on.\\n\\n## Status Page\\n\\nYes, we have a status page where you can check if everything is ok with our service.\\nIt can be found at [status.packit.dev](https://status.packit.dev/)\\nOn the page, you can find a list of incidents we resolved or are trying to fix.\\nIf you don\'t see any incident and still think the service isn\'t working as expected, please,\\nlet us know (see [contacts](/#contact)).\\nAnother useful source of information is the [pipelines view](https://dashboard.packit.dev/pipelines) on our dashboard.\\n\\n![Packit Status Page: systems](img/status-1.png)\\n![Packit Status Page: incidents](img/status-2.png)\\n\\n## Future\\n\\nAs you see, we managed to accomplish a lot last year. And what you can expect this year?\\n[Let us know]({{< ref \\"faq#how-can-i-contact-you\\" >}}) if you have some ideas and want to influence that!"},{"id":"/2021-in-numbers","metadata":{"permalink":"/posts/2021-in-numbers","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/2021-in-numbers/index.md","source":"@site/posts/2021-in-numbers/index.md","title":"2021 in Numbers","description":"Let\'s take a look on the year 2021 through some numbers.","date":"2022-01-04T00:00:00.000Z","formattedDate":"January 4, 2022","tags":[{"label":"2021","permalink":"/posts/tags/2021"},{"label":"yearly-numbers","permalink":"/posts/tags/yearly-numbers"},{"label":"summary","permalink":"/posts/tags/summary"}],"readingTime":1.395,"hasTruncateMarker":true,"authors":[{"name":"Franti\u0161ek Lachman","email":"flachman@redhat.com","url":"https://github.com/lachmanfrantisek","imageURL":"https://github.com/lachmanfrantisek.png","key":"flachman"}],"frontMatter":{"title":"2021 in Numbers","date":"2022-01-04T00:00:00.000Z","authors":"flachman","tags":["2021","yearly-numbers","summary"]},"prevItem":{"title":"2021 for Packit","permalink":"/posts/2021-features"},"nextItem":{"title":"Working on the next major RHEL release, in your upstream repo","permalink":"/posts/fedora-eln"}},"content":"Let\'s take a look on the year 2021 through some numbers.\\nWe would like to show you some interesting statistics and charts\\nthat can describe the work of Packit during the year 2021.\\nIf you are more interested in new features,\\nlet\'s take a look on our [second post](/posts/2021-features).\\n\\n\x3c!--truncate--\x3e\\n\\n## GitHub Application\\n\\nAs of now, we have `169` installations of our GitHub application and `41` of them is from the year 2021.\\nLooking at the monthly numbers below, it looks like we are getting back to shape.\\n\\n![GitHub installations in 2021](img/github-installations.png)\\n\\n## Builds\\n\\nCompared to the year 2020 when we made `28 430` Copr builds for our users,\\nwe made `4.6` times more in the year 2021: `133 222` Copr builds.\\nFor those who remember the start of our project,\\nwe had a goal of `5` thousand for the [FLOCK](https://flocktofedora.org/) 2019.\\nWe are now two digits ahead!\\nAnd if you are wondering how active is our user on Copr,\\nwe\'ve created `2/3` of all the new Copr projects during the year.\\n\\nTo made this happen, we\'ve created `36 133` source RPM files in the year 2021.\\n\\n![Copr builds in 2021](img/copr-builds.png)\\n\\n## Test runs\\n\\nSadly, we started saving the submit time of the test runs in June\\nso we have numbers only for the second half of the year.\\nThe numbers are not so high as for the builds but still `18 498` test runs.\\n\\n![Test runs in 2021](img/test-runs.png)\\n\\n## Top 20 projects in the number of PR Copr Builds\\n\\n![Top20 projects in the number of PR Copr Builds](img/top20-copr-builds.png)\\n\\n## Top 20 projects in the number of PR test runs\\n\\n![Top20 projects in the number of PR Test runs](img/top20-test-runs.png)"},{"id":"/fedora-eln","metadata":{"permalink":"/posts/fedora-eln","editUrl":"https://github.com/packit/packit.dev/tree/main/posts/fedora-eln/index.md","source":"@site/posts/fedora-eln/index.md","title":"Working on the next major RHEL release, in your upstream repo","description":"Fedora EL Ni\xf1o (ELN) is such an","date":"2020-10-04T00:00:00.000Z","formattedDate":"October 4, 2020","tags":[{"label":"downstream","permalink":"/posts/tags/downstream"}],"readingTime":2.29,"hasTruncateMarker":true,"authors":[{"name":"Tom\xe1\u0161 Tome\u010dek","email":"ttomecek@redhat.com","url":"https://github.com/TomasTomecek","imageURL":"https://github.com/TomasTomecek.png","key":"ttomecek"}],"frontMatter":{"title":"Working on the next major RHEL release, in your upstream repo","date":"2020-10-04T00:00:00.000Z","authors":"ttomecek","tags":["downstream"]},"prevItem":{"title":"2021 in Numbers","permalink":"/posts/2021-in-numbers"}},"content":"[Fedora EL Ni\xf1o](https://docs.fedoraproject.org/en-US/eln/) (ELN) is such an\\nawesome idea. It enables building rawhide packages in two distinct buildroots:\\n\\n1. the standard Fedora Rawhide buildroot and\\n2. a second one, which mimics Red Hat Enterprise Linux\\n\\nThis way you can make sure that your new upstream release builds fine in the\\nnext RHEL.\\n\\n\x3c!--truncate--\x3e\\n\\nBut this feedback might be a little bit too late: the upstream\\nrelease already happened and the code was imported in Fedora dist-git, so\\nfixing an issue will require repeating the whole process. Wouldn\'t it be better\\nto know if the upstream change builds fine in ELN **while** working on the\\ncode?\\n\\nOh, wait!\\n\\n### You can do this easily with Packit\\n\\nIf your GitHub project is not using Packit yet, [here\'s a\\nguide](https://packit.dev/docs/guide) how to start.\\n\\nOnce it\'s set up, you need to make sure that your pull requests are also being\\nbuilt in the `fedora-eln` target:\\n\\n```\\njobs:\\n  - job: copr_build\\n    trigger: pull_request\\n    metadata:\\n      targets:\\n        - fedora-development\\n        - fedora-eln\\n```\\n\\nWith this config, changes from every pull request will be built in all\\ndevelopment versions of Fedora (at the time of writing this, it\'s Rawhide and\\nFedora 33) and in Fedora ELN.\\n\\nEasy, right?\\n\\nPackit can also trigger builds when you push to a branch. If you want to have\\nup to date builds of your main branch for ELN and development versions of\\nFedora, here\'s how to set it up:\\n\\n```\\njobs:\\n  - job: copr_build\\n    trigger: commit\\n    metadata:\\n      targets:\\n        - fedora-development\\n        - fedora-eln\\n      branch: main\\n```\\n\\n### A real-life example\\n\\nIf you got here and you\'re still not sure why you\'d need this, I can give\\nyou a real-life example.\\n\\nRecently, [Jirka Konecny](https://github.com/jkonecny12) from the RHEL\\nInstaller team reached out to us that they would love to use Packit as a CI\\nsystem. He set it up and now all the anaconda PRs are being built and tested on\\n`Fedora Rawhide x86_64`.\\n\\n![Anaconda PR passing tests](img/anaconda-rawhide-tests-passing.png)\\n\\nJirka continued and added Fedora ELN as an additional target. The build failed\\nbecause one of build requirements was not available in ELN:\\n\\n```\\nFedora ELN - Developmental modular packages for the next Enterprise Linux release               2.7 kB/s | 2.3 kB     00:0\\nNo matching package to install: \'metacity\'\\nNot all dependencies satisfied\\nError: Some packages could not be found.\\n```\\n\\nSince the team discovered this during their upstream development process, they\\ncan react to the issue right away. It would have been pretty late if they found this\\nwhile the next major RHEL is reaching alpha - at this moment they should\\nhave enough time to fix the problem and make sure anaconda builds fine in ELN.\\n\\nSo, are you convinced? Let us know if you need help setting up Packit in your\\nupstream repositories :)"}]}')}}]);