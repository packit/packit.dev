"use strict";(self.webpackChunkpackit_dev=self.webpackChunkpackit_dev||[]).push([[94022],{15680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>g});var i=t(96540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=i.createContext({}),d=function(e){var n=i.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=d(e.components);return i.createElement(l.Provider,{value:n},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},h=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=d(t),h=a,g=u["".concat(l,".").concat(h)]||u[h]||c[h]||o;return t?i.createElement(g,r(r({ref:n},p),{},{components:t})):i.createElement(g,r({ref:n},p))}));function g(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,r=new Array(o);r[0]=h;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:a,r[1]=s;for(var d=2;d<o;d++)r[d]=t[d];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}h.displayName="MDXCreateElement"},38515:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),a=(t(96540),t(15680));const o={title:"Job referencing",authors:["mmassari"],sidebar_position:1},r="Job referencing",s={unversionedId:"job_referencing/index",id:"job_referencing/index",title:"Job referencing",description:"Already implemented use cases",source:"@site/research/job_referencing/index.md",sourceDirName:"job_referencing",slug:"/job_referencing/",permalink:"/research/job_referencing/",draft:!1,editUrl:"https://github.com/packit/research/tree/main/research/job_referencing/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Job referencing",authors:["mmassari"],sidebar_position:1},sidebar:"autogenerated",previous:{title:"Research notes",permalink:"/research/"},next:{title:"Making decisions in our projects",permalink:"/research/making-decisions/"}},l={},d=[{value:"Already implemented use cases",id:"already-implemented-use-cases",level:2},{value:"COPR build depends on SRMP build",id:"copr-build-depends-on-srmp-build",level:3},{value:"Tests depends on COPR builds",id:"tests-depends-on-copr-builds",level:3},{value:"New use cases",id:"new-use-cases",level:2},{value:"Tests depends on specified COPR builds",id:"tests-depends-on-specified-copr-builds",level:3},{value:"The easiest solution I see",id:"the-easiest-solution-i-see",level:4},{value:"<code>build_identifier</code> key name or?",id:"build_identifier-key-name-or",level:4},{value:"Monorepo: Package dependencies",id:"monorepo-package-dependencies",level:3},{value:"The easiest solution I see following what we are already doing for copr builds and tests",id:"the-easiest-solution-i-see-following-what-we-are-already-doing-for-copr-builds-and-tests",level:4},{value:"Request new feature: support side tag for multi package update",id:"request-new-feature-support-side-tag-for-multi-package-update",level:3},{value:"Drawbacks",id:"drawbacks",level:2},{value:"Conclusion",id:"conclusion",level:2}],p={toc:d},u="wrapper";function c(e){let{components:n,...t}=e;return(0,a.yg)(u,(0,i.A)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"job-referencing"},"Job referencing"),(0,a.yg)("h2",{id:"already-implemented-use-cases"},"Already implemented use cases"),(0,a.yg)("p",null,"The following dependencies use cases are not exposed to the user.\nThis are implementation details useful as a starting point for thinking about user defined dependencies."),(0,a.yg)("h3",{id:"copr-build-depends-on-srmp-build"},"COPR build depends on SRMP build"),(0,a.yg)("p",null,(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit-service/blob/16236087fb8071c83b3802464f3af504f4fb933e/packit_service/worker/handlers/copr.py#L136-L203"},"Here")," we build both the SRPM and the COPR packages (and the COPR package depends on the SRPM one).\nWe don't need any special data to be saved in the database, we just check for the existance of the SRPM build.\nThis code is triggered by a ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuildStartEvent")," event, this event is sent by COPR both for the SRPM package build and for the COPR package build, in this way we run twice the code and we can deal with the dependency."),(0,a.yg)("h3",{id:"tests-depends-on-copr-builds"},"Tests depends on COPR builds"),(0,a.yg)("p",null,(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit-service/blob/16236087fb8071c83b3802464f3af504f4fb933e/packit_service/worker/handlers/testing_farm.py#L202-L241"},"Here")," we check that a COPR build exist for every target we should run a test for.\nIf not build is found we run a ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuldHandler")," instance.\nFor every ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuildEndEvent")," the ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuildEndHandler")," is able to run again the ",(0,a.yg)("inlineCode",{parentName:"p"},"TestingFarmHandler")," if needed.\nNo special data is stored in the database, the COPR build is searched using: project name, commit sha, copr build owner and target."),(0,a.yg)("h2",{id:"new-use-cases"},"New use cases"),(0,a.yg)("h3",{id:"tests-depends-on-specified-copr-builds"},"Tests depends on specified COPR builds"),(0,a.yg)("p",null,"Since it is possible to add identifiers for jobs, we allow defining multiple Copr build jobs and multiple TF jobs. In that case, Packit Service doesn't know what Copr build job to use for the particular TF job."),(0,a.yg)("p",null,"For this purpose, we could introduce a new field for job configs which would allow the referencing, for example:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},'- job: copr_build\nidentifier: "build1"\ntrigger: pull_request\nactions: ...\n- job: copr_build\nidentifier: "build2"\ntrigger: pull_request\nactions: ...\n- job: tests\nidentifier: "tests2"\ntrigger: pull_request\nbuild_identifier: "build1"\n- job: tests\nidentifier: "tests2"\ntrigger: pull_request\nbuild_identifier: "build2"\n')),(0,a.yg)("h4",{id:"the-easiest-solution-i-see"},"The easiest solution I see"),(0,a.yg)("p",null,"I don't think we need anything more in the database to be able to retrieve the copr build id. We need the data given in the ",(0,a.yg)("inlineCode",{parentName:"p"},"copr_build")," job matching the specified ",(0,a.yg)("inlineCode",{parentName:"p"},"build_identifier")," name in the ",(0,a.yg)("inlineCode",{parentName:"p"},"tests")," job, plus all the data we are already using in the search like the ",(0,a.yg)("inlineCode",{parentName:"p"},"commit_sha"),".\nWe need to add the ",(0,a.yg)("inlineCode",{parentName:"p"},"build_identifier")," key in config.\nWe need to modify ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit-service/blob/16236087fb8071c83b3802464f3af504f4fb933e/packit_service/worker/handlers/testing_farm.py#L202-L241"},"this code")," in a way that it is able to retrieve the proper ",(0,a.yg)("inlineCode",{parentName:"p"},"build_id"),"."),(0,a.yg)("h4",{id:"build_identifier-key-name-or"},(0,a.yg)("inlineCode",{parentName:"h4"},"build_identifier")," key name or?"),(0,a.yg)("p",null,"Another suggestion was to use ",(0,a.yg)("inlineCode",{parentName:"p"},"after")," as a name for the key. I like it but our implementation, if made as specified above, will not be generic. For this reason I would rather prefer ",(0,a.yg)("inlineCode",{parentName:"p"},"after_build")," or ",(0,a.yg)("inlineCode",{parentName:"p"},"build_identifier")," to mark the fact that in the test job we are waiting for a build job."),(0,a.yg)("h3",{id:"monorepo-package-dependencies"},"Monorepo: Package dependencies"),(0,a.yg)("p",null,"The following is the ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit/issues/1903"},"RFE")," by LecrisUT"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"packages:\nA:\ndepends:\n- B\n- name: C\nrebuild: true\nB: ...\nC: ...\n")),(0,a.yg)("p",null,"at first I would simplify it like:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"packages:\nA:\ndepends:\n- B\n- C\nB: ...\nC: ...\n")),(0,a.yg)("p",null,"I would always rebuild everything for every package but we can enhance the code and if a package has no changes in the given ",(0,a.yg)("inlineCode",{parentName:"p"},"commit_sha")," and it does not depend on changed packages, we can simply copy lines from the previous database pipeline, filling the database as if the jobs were run."),(0,a.yg)("h4",{id:"the-easiest-solution-i-see-following-what-we-are-already-doing-for-copr-builds-and-tests"},"The easiest solution I see following what we are already doing for copr builds and tests"),(0,a.yg)("p",null,"We should ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit-service/blob/16236087fb8071c83b3802464f3af504f4fb933e/packit_service/worker/jobs.py#L464-L497"},"create tasks")," only for those packages that do not depend on other packages.\nWhen running, as an example, the COPR build for the latest dependent package in the list (C in our example), at the end of the handler we can create the COPR build tasks for package A (if any).\nThis can be done for all the kinds of job we directly start at ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit-service/blob/16236087fb8071c83b3802464f3af504f4fb933e/packit_service/worker/jobs.py#L464-L497"},"this lines"),"."),(0,a.yg)("p",null,"We can change ",(0,a.yg)("inlineCode",{parentName:"p"},"bodhi")," jobs in a way that they will do nothing, apart from starting new ",(0,a.yg)("inlineCode",{parentName:"p"},"bodhi")," tasks for other packages, unless they belong to a package not required by another package. Ideally only package A, in this example, should do a ",(0,a.yg)("inlineCode",{parentName:"p"},"bodhi")," update if I am not wrong. Or at least we should be able to skip doing something in this job during the chain of dependencies."),(0,a.yg)("p",null,"I think we don't need changes in the database schema to be able to implement this solution.\nWe need to add the ",(0,a.yg)("inlineCode",{parentName:"p"},"depends")," or ",(0,a.yg)("inlineCode",{parentName:"p"},"depends_on")," key to the schema. I slightly prefer the ",(0,a.yg)("inlineCode",{parentName:"p"},"depends")," key, but not strong opinions on that."),(0,a.yg)("h3",{id:"request-new-feature-support-side-tag-for-multi-package-update"},"Request new feature: support side tag for multi package update"),(0,a.yg)("p",null,"This is the ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit/issues/1870"},"RFE"),"."),(0,a.yg)("p",null,"I am not really sure I am getting the point here. But they are talking about a multi-package feature and for this reason I assume they need to use the monorepo syntax (otherwise we have no way to reference the packages).\nIf they can and want to use the monorepo syntax then I think that the package dependencies solution for the monorepos would solve their problems too. Or am I wrong?"),(0,a.yg)("h2",{id:"drawbacks"},"Drawbacks"),(0,a.yg)("p",null,"I see just a drawback in the previous solutions, the code is getting more and more complex to read (I mean to grasp the execution order) and test.\nI think ",(0,a.yg)("inlineCode",{parentName:"p"},"Celery")," was well suited for tasks which didn't depend on each other, now that dependencies are growing quickly I feel it is like a limit. I would like to be able to have communication between tasks and suspend the execution without exiting.\nI think that a code using ",(0,a.yg)("inlineCode",{parentName:"p"},"asyncio"),", as an example, could be more readable and testable."),(0,a.yg)("p",null,"Let's take as an example the simple dependency we have between COPR builds and SRPM builds"),(0,a.yg)("p",null,(0,a.yg)("a",{parentName:"p",href:"https://github.com/packit/packit-service/blob/16236087fb8071c83b3802464f3af504f4fb933e/packit_service/worker/handlers/copr.py#L136-L203"},"This code")," is performing both builds. But it is not straightforward to know that this code is called twice, because there are two ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuildStartEvent"),"(s) sent by the COPR server. The execution flow is not obvious and not easily testable, even though this is a really simple example."),(0,a.yg)("p",null,"I would like to be able to write something similar using ",(0,a.yg)("inlineCode",{parentName:"p"},"asyncio"),".\nAs far as I can understand it ",(0,a.yg)("a",{parentName:"p",href:"https://docs.temporal.io"},"temporalio")," would give us this flexibility. For this reason I wrote down some ",(0,a.yg)("em",{parentName:"p"},"python pseudo code")," using ",(0,a.yg)("inlineCode",{parentName:"p"},"temporalio"),". I don't really know it so I can be missing something."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'from temporalio.client import Client\n\nclient = await Client.connect("localhost:7233")\n\n# Run a worker for the workflow\nasync with Worker(\n  client,\n  task_queue="project-sha-event-queue",\n  workflows=[COPRBuildHandler],\n  activities=[submit_build_srpm,\n              submit_build_copr,\n              update_ux]\n  ):\n\n  # While the worker is running, use the client to start the workflow.\n  # Note, in many production setups, the client would be in a completely\n  # separate process from the worker.\n  result = await client.execute_workflow(\n    COPRBuildHandler.run,\n    id="copr-build-for-project-and-sha",\n    task_queue="project-sha-event-copr-build-queue",\n    execution_timeout=timedelta(minutes=60)\n')),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"from temporalio import workflow\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\n@activity.defn\nasync def submit_build_srpm():\n  ...\n\n@activity.defn\nasync def submit_build_copr():\n  ...\n\n@activity.defn\nasync def update_ux():\n  ...\n\nclass COPRBuildHandler:\n\n  def __init__(self) -> None:\n    self._srpm_builded = False\n    self._copr_builded = False\n\n  @reacts_to(event=CoprBuildEndEvent)\n  @workflow.signal\n  def submit_srpm_build_ended():\n    if event.chroot == COPR_SRPM_CHROOT:\n      self._srpm_builded = True\n\n  @reacts_to(event=CoprBuildEndEvent)\n  @workflow.signal\n  def submit_copr_build_ended():\n    if event.chroot != COPR_SRPM_CHROOT:\n      self._copr_builded = True\n\n  @workflow.run\n  async def run(self):\n\n    srpm_activity_handle = workflow.start_activity(\n      submit_build_srpm,\n      start_to_close_timeout=timedelta(seconds=1),\n      ...\n      )\n    await workflow.wait_condition(lambda: self._srpm_builded)\n\n    update_ux_handle = workflow.start_activity(\n      update_ux,\n      start_to_close_timeout=timedelta(seconds=1),\n      ...\n      )\n\n    build_copr_handle = workflow.start_activity(\n      build_copr,\n      start_to_close_timeout=timedelta(seconds=1),\n      ...\n      )\n    await workflow.wait_condition(lambda: self._copr_builded)\n\n    update_ux_handle = workflow.start_activity(\n      update_ux,\n      start_to_close_timeout=timedelta(seconds=1),\n      ...\n      )\n")),(0,a.yg)("p",null,"As far as I understand from the ",(0,a.yg)("inlineCode",{parentName:"p"},"temporalio")," documentation, the framework is in charge of putting this ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuildHandler workflow")," in a sleeping queue while it is waiting for a signal and waking up it later. So we don't need to exit the workflow and the order of the activities is well visible in the ",(0,a.yg)("inlineCode",{parentName:"p"},"workflow")," code."),(0,a.yg)("p",null,"There is another small improvement I see using a solution like this one instead of doing what we are already doing.\nI don't think that the ",(0,a.yg)("inlineCode",{parentName:"p"},"CoprBuildEndHandler")," should deal with the ",(0,a.yg)("inlineCode",{parentName:"p"},"TestingFarmHandler")," and viceversa, this is dangerous, and the more dependencies we will introduce the more we will need this dangerous cross referencing."),(0,a.yg)("p",null,"Something above them should be able to orchestrate and know the jobs, and I think a ",(0,a.yg)("inlineCode",{parentName:"p"},"workflow")," is exactly what we are missing here."),(0,a.yg)("h2",{id:"conclusion"},"Conclusion"),(0,a.yg)("p",null,"From my point of view we have two possible ways to implement job referencing:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("p",{parentName:"li"},"We can keep doing what we are already doing: use Celery and spawn different tasks when we need them."),(0,a.yg)("p",{parentName:"li"},"PRO: this is the quickest solution"),(0,a.yg)("p",{parentName:"li"},"CON: it does not scale well, every new reference makes our code less readable/testeable and makes managing errors harder")),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("p",{parentName:"li"},"Introduce a workflow engine with support for tasks communication"),(0,a.yg)("p",{parentName:"li"},"PRO: it should make our code more readable/testeable and we should be able to better manage error handling"),(0,a.yg)("p",{parentName:"li"},"CON: it will take us far more time"))),(0,a.yg)("p",null,"An hybrid solution could be: go with solution 1 and implement just those job referencing we believe are actually/really needed.\nIf we realize we would need other job referencing in future, we want to make job referencing more generic or we want the user to be able to build his own workflow then we need to start studying a workflow manager (like temporal) and planning a change in our core code base to go with solution 2. We will need to adjust the code and all the ",(0,a.yg)("strong",{parentName:"p"},"configuration keys"),", used with solution 1, when we will decide to go with solution 2."))}c.isMDXComponent=!0}}]);